# Run files

These tests examine the files generated by a run.

We'l use the `batch` sample project for our tests.

    >>> project = Project(sample("projects", "batch"))

Helper to print files. The function skips printing any `vcs_commit`
attr files as these are generated inconsistently based on whether
tests are run from source or from an installed Guild package.

    >>> def print_files(files):
    ...     for file in files:
    ...         if not file.endswith("vcs_commit"):
    ...             print(file)

We use an environment for runs that reverts the disabling of pip
freeze.

    >>> PipFreezeEnabled = Env({"NO_PIP_FREEZE": "0"})

## Normal runs

A simple run that prints a message:

    >>> with PipFreezeEnabled:
    ...     project.run("say.py", flags={"msg": "hi"})
    hi

Our runs:

    >>> runs = project.list_runs()
    >>> len(runs)
    1

The files generated for our run:

    >>> first_run = runs[0]
    >>> print_files(project.ls(first_run, all=True))  # doctest: +REPORT_UDIFF
    .guild/attrs/cmd
    .guild/attrs/deps
    .guild/attrs/env
    .guild/attrs/exit_status
    .guild/attrs/flags
    .guild/attrs/host
    .guild/attrs/id
    .guild/attrs/initialized
    .guild/attrs/label
    .guild/attrs/op
    .guild/attrs/pip_freeze
    .guild/attrs/platform
    .guild/attrs/random_seed
    .guild/attrs/run_params
    .guild/attrs/sourcecode_digest
    .guild/attrs/started
    .guild/attrs/stopped
    .guild/attrs/user
    .guild/attrs/user_flags
    .guild/opref
    .guild/output
    .guild/output.index
    .guild/sourcecode/add.py
    .guild/sourcecode/batch-2.csv
    .guild/sourcecode/batch.csv
    .guild/sourcecode/batch.json
    .guild/sourcecode/batch.unknown
    .guild/sourcecode/batch.yaml
    .guild/sourcecode/batch.yml
    .guild/sourcecode/error.py
    .guild/sourcecode/guild.yml
    .guild/sourcecode/invalid-data.json
    .guild/sourcecode/invalid-data.yml
    .guild/sourcecode/invalid-item.json
    .guild/sourcecode/invalid-item.yaml
    .guild/sourcecode/say.py
    .guild/sourcecode/single-run-2.json
    .guild/sourcecode/single-run.csv
    .guild/sourcecode/single-run.json
    .guild/sourcecode/tune.py

Let's look at some of the generated files.

NOTE: We intentionally skip env, started, stopped, and output.index
files.

    >>> project.cat(first_run, ".guild/attrs/cmd")
    - ...
    - -um
    - guild.op_main
    - say
    - --msg
    - hi

    >>> project.cat(first_run, ".guild/attrs/deps")
    {}

    >>> project.cat(first_run, ".guild/attrs/exit_status")
    0

    >>> project.cat(first_run, ".guild/attrs/flags")
    loud: false
    msg: hi

    >>> project.cat(first_run, ".guild/opref")
    script:.../samples/projects/batch... '' '' say.py

    >>> project.cat(first_run, ".guild/output")
    hi

    >>> project.cat(first_run, ".guild/attrs/platform")
    architecture: ...
    cpus: ...
    processor: ...
    python_version: ...
    uname: ...

## Batch runs

Batch runs are normal runs and so have a similar list of
files. However, they also contain a `proto` run, which serves to
generate batch trials.

Let's run a batch operation to illustrate. We can indicate a run
should be a batch by specifying a list of values for a flag.

    >>> with PipFreezeEnabled:
    ...     project.run("say.py", flags={"msg": ["ho"]}, keep_batch=True)
    INFO: [guild] Running trial ...: say.py (loud=no, msg=ho)
    ho

The batch generates two runs, one for the batch and the other for the
trial (the third run is from our previous test):

    >>> runs = project.list_runs()
    >>> len(runs)
    3

The latest run is the trial:

    >>> trial_run = runs[0]
    >>> trial_run.opref.op_name
    'say.py'

    >>> print_files(project.ls(trial_run, all=True))  # doctest: +REPORT_UDIFF
    .guild/attrs/cmd
    .guild/attrs/deps
    .guild/attrs/env
    .guild/attrs/exit_status
    .guild/attrs/flags
    .guild/attrs/host
    .guild/attrs/id
    .guild/attrs/initialized
    .guild/attrs/label
    .guild/attrs/op
    .guild/attrs/pip_freeze
    .guild/attrs/platform
    .guild/attrs/random_seed
    .guild/attrs/run_params
    .guild/attrs/sourcecode_digest
    .guild/attrs/started
    .guild/attrs/stopped
    .guild/attrs/user
    .guild/attrs/user_flags
    .guild/opref
    .guild/output
    .guild/output.index
    .guild/sourcecode/add.py
    .guild/sourcecode/batch-2.csv
    .guild/sourcecode/batch.csv
    .guild/sourcecode/batch.json
    .guild/sourcecode/batch.unknown
    .guild/sourcecode/batch.yaml
    .guild/sourcecode/batch.yml
    .guild/sourcecode/error.py
    .guild/sourcecode/guild.yml
    .guild/sourcecode/invalid-data.json
    .guild/sourcecode/invalid-data.yml
    .guild/sourcecode/invalid-item.json
    .guild/sourcecode/invalid-item.yaml
    .guild/sourcecode/say.py
    .guild/sourcecode/single-run-2.json
    .guild/sourcecode/single-run.csv
    .guild/sourcecode/single-run.json
    .guild/sourcecode/tune.py

The next run is the batch:

    >>> batch_run = runs[1]
    >>> batch_run.opref.op_name
    '+'

Its files:

    >>> print_files(project.ls(batch_run, all=True))  # doctest: +REPORT_UDIFF
    .guild/attrs/cmd
    .guild/attrs/deps
    .guild/attrs/env
    .guild/attrs/exit_status
    .guild/attrs/flags
    .guild/attrs/host
    .guild/attrs/id
    .guild/attrs/initialized
    .guild/attrs/objective
    .guild/attrs/op
    .guild/attrs/pip_freeze
    .guild/attrs/platform
    .guild/attrs/random_seed
    .guild/attrs/run_params
    .guild/attrs/sourcecode_digest
    .guild/attrs/started
    .guild/attrs/stopped
    .guild/attrs/user
    .guild/attrs/user_flags
    .guild/opref
    .guild/output
    .guild/output.index
    .guild/proto/.guild/PENDING
    .guild/proto/.guild/attrs/cmd
    .guild/proto/.guild/attrs/flags
    .guild/proto/.guild/attrs/host
    .guild/proto/.guild/attrs/id
    .guild/proto/.guild/attrs/initialized
    .guild/proto/.guild/attrs/label
    .guild/proto/.guild/attrs/op
    .guild/proto/.guild/attrs/pip_freeze
    .guild/proto/.guild/attrs/platform
    .guild/proto/.guild/attrs/random_seed
    .guild/proto/.guild/attrs/run_params
    .guild/proto/.guild/attrs/sourcecode_digest
    .guild/proto/.guild/attrs/user
    .guild/proto/.guild/attrs/user_flags
    .guild/proto/.guild/opref
    .guild/proto/.guild/sourcecode/add.py
    .guild/proto/.guild/sourcecode/batch-2.csv
    .guild/proto/.guild/sourcecode/batch.csv
    .guild/proto/.guild/sourcecode/batch.json
    .guild/proto/.guild/sourcecode/batch.unknown
    .guild/proto/.guild/sourcecode/batch.yaml
    .guild/proto/.guild/sourcecode/batch.yml
    .guild/proto/.guild/sourcecode/error.py
    .guild/proto/.guild/sourcecode/guild.yml
    .guild/proto/.guild/sourcecode/invalid-data.json
    .guild/proto/.guild/sourcecode/invalid-data.yml
    .guild/proto/.guild/sourcecode/invalid-item.json
    .guild/proto/.guild/sourcecode/invalid-item.yaml
    .guild/proto/.guild/sourcecode/say.py
    .guild/proto/.guild/sourcecode/single-run-2.json
    .guild/proto/.guild/sourcecode/single-run.csv
    .guild/proto/.guild/sourcecode/single-run.json
    .guild/proto/.guild/sourcecode/tune.py
    ...

Note the additional `proto` directory. This contains the prototype for
the `say.py` operation, which is used by the batch operation to
generate each specific trial op.

Note also the last entry '...' - this is the link to the trial run
directory.
