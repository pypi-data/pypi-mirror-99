Metadata-Version: 2.1
Name: WeightedEnsemble
Version: 1.0.0
Summary: will help in getting the correct classification model
Home-page: UNKNOWN
Author: Thanmay Jasti
License: UNKNOWN
Platform: UNKNOWN
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.6

Classification.Weighted_Ensemble(Models,X_train,y_train,X_val,y_val,X_test,y_test,Method='SLSQP')

Any classification model  or an average Ensembling model combines the prediction from each model equally and often results in better performance on average than a given single model.

Weighted Ensemble model is an approach that allows multiple models to contribute to a prediction in proportion to their trust or estimated performance.

Using this above mentoined Weighted_Ensemble class You can predict single model as well as multiple models Which will give  the clarity about the weightage of the model with respect to inputs 

Parameters:
Models==List of the model objects that you wanted to Predict 
X_train==Traininig dataset
y_train==Training label dataset 
x_val==Validation data set
Y_val==Validation labels dataset
#X_val and y_Val are used to calculate the weights of the model objects that are there in the Models List 
NOte:The sum of the weights of the modes mentioned in the object are always equal to 1
x_test==Testing dataset to calculate the metric score
Y_test==Testing label dataset to calculate the metric score

Example 
in the code  declare models=[]
Model1:
rf=Randomclassifier()
models.append(rf)

Model2:
xgb=XGBClassifier()
models.append(xgb)

result,weights=Weighted_Ensemble(Models,X_train,y_train,X_val,y_val,X_test,y_test,Method='SLSQP')

Result is a data frame which is something like below
threshold	tp	fp	tn	fn	accuracy	precision	recall	scope	f1	fpr
Threshold values range from o.1 to 0.99 
Weights  are the list of values that shows how much weightage it has given to the model 
We can compare the values and pick the model very easily 

Classification.get_metric_score(y_true, y_proba):
If we have model already in hand we can call this method to observe the  perfomance of the model  with various thershold values 
Result is a data frame which is something like below
threshold	tp	fp	tn	fn	accuracy	precision	recall	scope	f1	fpr
Threshold values range from o.1 to 0.99  

Parameters:
y_true==actual y values
y_proba==We should give the probability values with respect to the predcition once

Example:
y_true is the actual values
Y_proba=np.round(model.predict_proba(X_test)[:,1],3)
result_df=get_metric_score(y_true, y_proba)




