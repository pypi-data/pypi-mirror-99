{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking with sktime\n",
    "\n",
    "The benchmarking modules allows you to easily orchestrate benchmarking experiments in which you want to compare the performance of one or more algorithms over one or more data sets. It also provides a number of statistical tests to check if observed performance differences are statistically significant. \n",
    "\n",
    "The benchmarking modules is based on [mlaut](https://github.com/alan-turing-institute/mlaut)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sktime.benchmarking.strategies'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f5c75c3ce0ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msktime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmarking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morchestration\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOrchestrator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msktime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmarking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHDDResults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msktime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmarking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategies\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTSCStrategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msktime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmarking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTSCTask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sktime.benchmarking.strategies'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sktime.benchmarking.data import UEADataset, make_datasets\n",
    "from sktime.benchmarking.evaluation import Evaluator\n",
    "from sktime.benchmarking.metrics import PairwiseMetric\n",
    "from sktime.benchmarking.orchestration import Orchestrator\n",
    "from sktime.benchmarking.results import HDDResults\n",
    "from sktime.benchmarking.strategies import TSCStrategy\n",
    "from sktime.benchmarking.tasks import TSCTask\n",
    "\n",
    "from sktime.classification.interval_based import TimeSeriesForest\n",
    "\n",
    "from sktime.series_as_features.model_selection import PresplitFilesCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set paths to data and results folder\n",
    "import sktime\n",
    "DATAPATH = os.path.join(os.path.dirname(sktime.__file__), \"datasets/data\")\n",
    "RESULTSPATH = \"results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pointers to datasets on hard drive\n",
    "Here we use the `UEADataset` which follows the [UEA/UCR format](http://www.timeseriesclassification.com) and some of the time series classification datasets included in sktime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create individual pointers to dataset on the disk\n",
    "datasets = [\n",
    "    UEADataset(path=DATAPATH, name=\"GunPoint\"),\n",
    "    UEADataset(path=DATAPATH, name=\"ItalyPowerDemand\"),\n",
    "    UEADataset(path=DATAPATH, name=\"ArrowHead\")\n",
    "]\n",
    "\n",
    "# Alternatively, we can use a helper function to create them automatically\n",
    "datasets = make_datasets(path=DATAPATH, dataset_cls=UEADataset, \n",
    "                         names=[\"GunPoint\", \"ItalyPowerDemand\", \"ArrowHead\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each dataset, we also need to specify a learning task\n",
    "In this case, all tasks are the same, because the target variable has the same name for all datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [TSCTask(target=\"target\") for _ in range(len(datasets))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify learning strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mloning/.conda/envs/sktime-dev/lib/python3.7/site-packages/sklearn/base.py:197: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Specify learning strategies\n",
    "strategies = [\n",
    "    TSCStrategy(TimeSeriesForestClassifier(n_estimators=10), name=\"tsf10\"),\n",
    "    TSCStrategy(TimeSeriesForestClassifier(n_estimators=20), name=\"tsf20\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a results object\n",
    "The results object encapsulates where and how benchmarking results are stored, here we choose to output them to the hard drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify results object which manages the output of the benchmarking\n",
    "results = HDDResults(path=RESULTSPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mloning/.conda/envs/sktime-dev/lib/python3.7/site-packages/sklearn/base.py:197: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  FutureWarning)\n",
      "/Users/mloning/.conda/envs/sktime-dev/lib/python3.7/site-packages/sklearn/base.py:197: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  FutureWarning)\n",
      "/Users/mloning/.conda/envs/sktime-dev/lib/python3.7/site-packages/sklearn/base.py:197: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  FutureWarning)\n",
      "/Users/mloning/.conda/envs/sktime-dev/lib/python3.7/site-packages/sklearn/base.py:197: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  FutureWarning)\n",
      "/Users/mloning/.conda/envs/sktime-dev/lib/python3.7/site-packages/sklearn/base.py:197: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# run orchestrator\n",
    "orchestrator = Orchestrator(datasets=datasets,\n",
    "                            tasks=tasks,  \n",
    "                            strategies=strategies, \n",
    "                            cv=PresplitFilesCV(), \n",
    "                            results=results)\n",
    " \n",
    "orchestrator.fit_predict(save_fitted_strategies=False, overwrite_predictions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate and compare results\n",
    "Having run the orchestrator, we can evaluate and compare the prediction strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>strategy</th>\n",
       "      <th>accuracy_mean</th>\n",
       "      <th>accuracy_stderr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tsf10</td>\n",
       "      <td>0.870884</td>\n",
       "      <td>0.021333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tsf20</td>\n",
       "      <td>0.879359</td>\n",
       "      <td>0.018483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  strategy  accuracy_mean  accuracy_stderr\n",
       "0    tsf10       0.870884         0.021333\n",
       "1    tsf20       0.879359         0.018483"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = Evaluator(results)\n",
    "metric = PairwiseMetric(func=accuracy_score, name=\"accuracy\")\n",
    "metrics_by_strategy = evaluator.evaluate(metric=metric)\n",
    "metrics_by_strategy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The evaluator offers a number of additional methods for evaluating and comparing strategies, including statistical hypothesis tests and visualisation tools, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>strategy</th>\n",
       "      <th>accuracy_mean_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tsf10</td>\n",
       "      <td>1.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tsf20</td>\n",
       "      <td>1.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  strategy  accuracy_mean_rank\n",
       "0    tsf10            1.333333\n",
       "1    tsf20            1.666667"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.rank()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "Currently, the following functions are implemented:\n",
    "\n",
    "* `analyse.plot_boxplots()`\n",
    "* `analyse.ranks()`\n",
    "* `analyse.t_test()`\n",
    "* `analyse.sign_test()`\n",
    "* `analyse.ranksum_test()`\n",
    "* `analyse.t_test_with_bonferroni_correction()`\n",
    "* `analyse.wilcoxon_test()`\n",
    "* `analyse.friedman_test()`\n",
    "* `analyse.nemenyi()`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
