# -*- coding: utf-8 -*-
# This file is part of the pylint-ignore project
# https://github.com/mbarkhau/pylint-ignore
#
# Copyright (c) 2020-2020 Manuel Barkhau (mbarkhau@gmail.com) - MIT License
# SPDX-License-Identifier: MIT
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
from __future__ import unicode_literals
import re
import shutil
import typing as typ
import hashlib
import logging
import collections
import pylev
try:
    import builtins
except ImportError:
    import __builtin__ as builtins
import pathlib2 as pl
range = getattr(builtins, 'xrange', range)
str = getattr(builtins, 'unicode', str)
logger = logging.getLogger('pylint_ignore')
IGNOREFILE_HEADER = """# Pylint-Ignore

**WARNING: This file is programatically generated.**

This file is parsed by [`pylint-ignore`](https://pypi.org/project/pylint-ignore/)
to determine which
[Pylint messages](https://pylint.pycqa.org/en/stable/technical_reference/features.html)
should be ignored.

- Do not edit this file manually.
- To update, use `pylint-ignore --update-ignorefile`

The recommended approach to using `pylint-ignore` is:

1. If a message refers to a valid issue, update your code rather than
   ignoring the message.
2. If a message should *always* be ignored (globally), then to do so
   via the usual `pylintrc` or `setup.cfg` files rather than this
   `pylint-ignore.md` file.
3. If a message is a false positive, add a comment of this form to your code:
   `# pylint:disable=<symbol> ; explain why this is a false positive`

"""
ENTRY_TEMPLATE = """
## File {entry.path} - {lineno_txt}{entry.msgid} ({entry.symbol})

- `message: {msg_text}`
- `author : {entry.author}`
- `date   : {entry.date}`

{ctx_src_text}
"""
_ENTRY_HEADER_PATTERN = """
^
    \\#\\#\\s
    File\\s(?P<path>.*?)
    \\s-\\s
    (?:
        Line\\s(?P<lineno>\\d+)
        \\s-\\s
    )?
    (?P<msgid>\\w\\d+)
    \\s
    \\((?P<symbol>.*)\\)
$
"""
ENTRY_HEADER_RE = re.compile(_ENTRY_HEADER_PATTERN, flags=re.VERBOSE)
_LIST_ITEM_PATTERN = """
^
\\s*-\\s
`
(?P<key>message|author|date)
\\s*:\\s
(?P<value>.*)
`
$
"""
LIST_ITEM_RE = re.compile(_LIST_ITEM_PATTERN, flags=re.VERBOSE)
_SOURCE_TEXT_PATTERN = """
(```|~~~)(?P<language>\\w+)?
    (
        (?:\\s+(?P<def_lineno>\\d+):\\s(?P<def_line>.*))?
        \\s+\\.\\.\\.
    )?
    (?:\\s+\\d+:\\s?.*)?
    (?:\\s+\\d+:\\s?.*)?
    \\s*\\>\\s+(?P<source_lineno>\\d+):\\s(?P<source_line>.*)
    (?:\\s+\\d+:\\s?.*)?
    (?:\\s+\\d+:\\s?.*)?
    \\s*
(```|~~~)
"""
SOURCE_TEXT_RE = re.compile(_SOURCE_TEXT_PATTERN, flags=re.VERBOSE)
SourceText = typ.NamedTuple('SourceText', [('new_lineno', int), (
    'old_lineno', int), ('source_line', str), ('text', str), ('start_idx',
    int), ('end_idx', int), ('def_line_idx', typ.Optional[int]), (
    'def_line', typ.Optional[str])])
MaybeSourceText = typ.Optional[SourceText]
Key = typ.NamedTuple('Key', [('msgid', str), ('path', str), ('symbol', str),
    ('msg_text', str), ('source_line', str)])
Entry = typ.NamedTuple('Entry', [('msgid', str), ('path', str), ('symbol',
    str), ('msg_text', str), ('msg_extra', str), ('author', str), ('date',
    str), ('srctxt', MaybeSourceText)])


class ObsoleteEntry(Exception):
    pass


Catalog = typ.Dict[Key, Entry]
FUZZY_MATCH_MAX_EDIT_DISTANCE_ABS = 8
FUZZY_MATCH_MAX_EDIT_DISTANCE_PCT = 20


def _iter_candidate_keys(catalog, search_key):
    for key in catalog.keys():
        is_candidate = (search_key.msgid == key.msgid and search_key.path ==
            key.path and search_key.symbol == key.symbol)
        if is_candidate:
            yield key


def _iter_fuzzy_entries(catalog, search_key):
    for key in _iter_candidate_keys(catalog, search_key):
        msg_text_dist = pylev.levenshtein(key.msg_text, search_key.msg_text)
        src_line_dist = pylev.levenshtein(key.source_line, search_key.
            source_line)
        if msg_text_dist > FUZZY_MATCH_MAX_EDIT_DISTANCE_ABS:
            continue
        if src_line_dist > FUZZY_MATCH_MAX_EDIT_DISTANCE_ABS:
            continue
        msg_text_dist_pct = 100 * msg_text_dist / max(len(key.msg_text),
            len(search_key.msg_text))
        src_line_dist_pct = 100 * src_line_dist / max(len(key.source_line),
            len(search_key.source_line))
        if msg_text_dist_pct > FUZZY_MATCH_MAX_EDIT_DISTANCE_PCT:
            continue
        if src_line_dist_pct > FUZZY_MATCH_MAX_EDIT_DISTANCE_PCT:
            continue
        yield catalog[key]


def find_entry(catalog, search_key):
    has_exact_match = search_key in catalog
    if has_exact_match:
        return catalog[search_key]
    matches = list(_iter_fuzzy_entries(catalog, search_key))
    if len(matches) == 1:
        return matches[0]
    else:
        return None


CONTEXT_LINES = 2
_SRC_CACHE = {}


def read_source_lines(path):
    if path not in _SRC_CACHE:
        if len(_SRC_CACHE) > 2:
            _SRC_CACHE.popitem()
        with pl.Path(path).open(mode='r', encoding='utf-8') as fobj:
            full_src_text = fobj.read()
        _keepends = True
        lines = full_src_text.splitlines(_keepends)
        _SRC_CACHE[path] = lines
    return _SRC_CACHE[path]


def find_source_text_lineno(path, old_source_line, old_lineno):
    if not pl.Path(path).exists():
        raise ObsoleteEntry('file not found')
    old_line_idx = old_lineno - 1
    lines = read_source_lines(path)
    for offset in range(100):
        for line_idx in {old_line_idx - offset, old_line_idx + offset}:
            is_matching_line = 0 <= line_idx < len(lines) and lines[line_idx
                ].rstrip() == old_source_line.rstrip()
            if is_matching_line:
                return line_idx + 1
    raise ObsoleteEntry('source text not found')


def read_source_text(path, new_lineno, old_lineno):
    lines = read_source_lines(path)
    line_idx = new_lineno - 1
    line_indent_lvl = len(lines[line_idx]) - len(lines[line_idx].lstrip())
    start_idx = max(0, line_idx - CONTEXT_LINES)
    end_idx = min(len(lines), line_idx + CONTEXT_LINES + 1)
    src_lines = lines[start_idx:end_idx]
    src_text = ''.join(src_lines)
    source_line = lines[line_idx]
    def_line_idx = None
    def_line = None
    maybe_def_idx = line_idx
    while maybe_def_idx > 0:
        line_text = lines[maybe_def_idx]
        indent_lvl = len(line_text) - len(line_text.lstrip())
        if line_text.strip() and indent_lvl < line_indent_lvl:
            first_token = line_text.lstrip().split()[0]
            if first_token in ('def', 'class'):
                is_defline_before_ctx_src = 0 <= maybe_def_idx < start_idx
                if is_defline_before_ctx_src:
                    def_line_idx = maybe_def_idx
                    def_line = lines[maybe_def_idx]
                break
        maybe_def_idx -= 1
    return SourceText(new_lineno, old_lineno, source_line, src_text,
        start_idx, end_idx, def_line_idx, def_line)


EntryValues = typ.Dict[str, str]


def _init_entry_item(entry_vals):
    msg_extra = ''
    if 'ctx_src_text' in entry_vals:
        old_ctx_src_text = entry_vals['ctx_src_text']
        old_source_text_match = SOURCE_TEXT_RE.match(old_ctx_src_text)
        if old_source_text_match is None:
            old_source_line = ''
            msg_extra = old_ctx_src_text.strip()[3:][:-3].strip()
        else:
            old_source_line = old_source_text_match.group('source_line')
    else:
        old_source_line = ''
    path = entry_vals['path']
    srctxt = None
    if entry_vals['lineno']:
        old_lineno = int(entry_vals['lineno'])
        try:
            new_lineno = find_source_text_lineno(path, old_source_line,
                old_lineno)
            srctxt = read_source_text(path, new_lineno, old_lineno)
            source_line = srctxt.source_line
        except ObsoleteEntry:
            source_line = old_source_line
    else:
        source_line = hashlib.sha1(msg_extra.encode('utf-8')).hexdigest()
    ignorefile_entry = Entry(entry_vals['msgid'], entry_vals['path'],
        entry_vals['symbol'], entry_vals['message'], msg_extra, entry_vals[
        'author'], entry_vals['date'], srctxt)
    ignorefile_key = Key(ignorefile_entry.msgid, ignorefile_entry.path,
        ignorefile_entry.symbol, ignorefile_entry.msg_text, source_line)
    return ignorefile_key, ignorefile_entry


def dumps_entry(entry):
    srctxt = entry.srctxt
    if srctxt is None:
        lineno = ''
        if '\n' in entry.msg_text:
            msg_text_parts = entry.msg_text.split('\n', 1)
            msg_text = msg_text_parts[0]
            ctx_src_text = msg_text_parts[1]
        else:
            msg_text = entry.msg_text
            ctx_src_text = entry.msg_extra or ''
    else:
        lineno = str(srctxt.new_lineno)
        last_ctx_lineno = srctxt.end_idx + 1
        padding_size = len(str(last_ctx_lineno))
        src_lines = []
        def_line = srctxt.def_line
        def_line_idx = srctxt.def_line_idx
        if def_line and def_line_idx:
            def_lineno = def_line_idx + 1
            line = def_line.rstrip()
            src_lines.append('  {0:>{1}}: {2}'.format(def_lineno,
                padding_size, line))
            if def_lineno + CONTEXT_LINES < srctxt.new_lineno:
                src_lines.append('  ...')
        for offset, line in enumerate(srctxt.text.splitlines()):
            src_lineno = str(srctxt.start_idx + offset + 1)
            padded_line = ' ' + line if line.strip() else ''
            if lineno == src_lineno:
                dumps_line = '> {0:>{1}}:{2}'.format(src_lineno,
                    padding_size, padded_line)
            else:
                dumps_line = '  {0:>{1}}:{2}'.format(src_lineno,
                    padding_size, padded_line)
            src_lines.append(dumps_line)
        msg_text = entry.msg_text
        ctx_src_text = '\n'.join(src_lines)
    if ctx_src_text.strip():
        ctx_src_text = '```\n' + ctx_src_text.rstrip() + '\n```\n\n'
    lineno_txt = 'Line {0} - '.format(lineno) if lineno else ''
    entry_text = ENTRY_TEMPLATE.format(entry=entry, lineno_txt=lineno_txt,
        msg_text=msg_text, ctx_src_text=ctx_src_text)
    return entry_text.lstrip('\n')


def _parse_ctx_src_text(fence, lines):
    ctx_src_text_lines = [fence + '\n']
    while True:
        _, next_line = next(lines)
        ctx_src_text_lines.append(next_line)
        is_close_fence = next_line.strip() == fence
        if is_close_fence:
            break
    return ''.join(ctx_src_text_lines)


def _iter_entry_values(ignorefile_path):
    entry_vals = {}
    with ignorefile_path.open(mode='r', encoding='utf-8') as fobj:
        lines = iter(enumerate(fobj))
        try:
            while True:
                i, line = next(lines)
                ignorefile_lineno = i + 1
                if line.startswith('```'):
                    fence = line[:3]
                    entry_vals['ctx_src_text'] = _parse_ctx_src_text(fence,
                        lines)
                    continue
                entry_header = ENTRY_HEADER_RE.match(line)
                if entry_header and 'msgid' in entry_vals:
                    yield entry_vals
                    entry_vals = {}
                if entry_header:
                    entry_vals['ignorefile_lineno'] = str(ignorefile_lineno)
                    entry_vals.update(entry_header.groupdict())
                    assert 'msgid' in entry_vals
                    continue
                list_item = LIST_ITEM_RE.match(line)
                if list_item:
                    entry_vals[list_item.group('key')] = list_item.group(
                        'value')
        except StopIteration:
            pass
    if 'msgid' in entry_vals:
        yield entry_vals


MESSAGE_TYPE_PRIORITIES = ['F', 'E', 'W', 'C', 'R', 'I']
assert MESSAGE_TYPE_PRIORITIES.index('F') < MESSAGE_TYPE_PRIORITIES.index('I')


def _entry_priority(entry):
    msg_type_priority = MESSAGE_TYPE_PRIORITIES.index(entry.msgid[:1])
    if entry.srctxt is None:
        new_lineno = 0
    else:
        new_lineno = entry.srctxt.new_lineno
    return msg_type_priority, entry.msgid, new_lineno, entry.msg_text


def dumps(ignorefile):
    if len(ignorefile) == 0:
        return IGNOREFILE_HEADER
    entries = sorted(ignorefile.values(), key=_entry_priority)
    msgid_count = collections.Counter(e.msgid for e in entries)
    overview_chunks = [IGNOREFILE_HEADER, '\n# Overview\n\n']
    entry_chunks = []
    prev_msg_id = None
    for entry in entries:
        if entry.msgid != prev_msg_id:
            prev_msg_id = entry.msgid
            num_entries = msgid_count[entry.msgid]
            section_text = '{0}: {1} ({2}x)'.format(entry.msgid, entry.
                symbol, num_entries)
            entry_link = '#{0}-{1}'.format(entry.msgid, entry.symbol).lower()
            section = ' - [{0}]({1})\n'.format(section_text, entry_link)
            overview_chunks.append(section)
            entry_chunks.append('# {0}: {1}\n\n'.format(entry.msgid, entry.
                symbol))
        entry_chunks.append(dumps_entry(entry))
    overview_chunks.append('\n\n')
    chunks = overview_chunks + entry_chunks
    return ''.join(chunks)


def dump(ignorefile, ignorefile_path):
    ignorefile_text = dumps(ignorefile)
    tmp_path = ignorefile_path.parent / (ignorefile_path.name + '.tmp')
    with tmp_path.open(mode='w', encoding='utf-8') as fobj:
        fobj.write(ignorefile_text)
    shutil.move(str(tmp_path), str(ignorefile_path))


def load(ignorefile_path):
    if not ignorefile_path.exists():
        return {}
    catalog = collections.OrderedDict()
    for entry_vals in _iter_entry_values(ignorefile_path):
        try:
            ignorefile_key, ignorefile_entry = _init_entry_item(entry_vals)
            catalog[ignorefile_key] = ignorefile_entry
        except ObsoleteEntry:
            pass
        except (KeyError, ValueError) as ex:
            lineno = entry_vals['ignorefile_lineno']
            path = entry_vals['path']
            logmsg = 'Error parsing entry on line {0} of {1}: {2}'.format(
                lineno, path, ex)
            logger.error(logmsg, exc_info=True)
    return catalog


def load_dir(dirpath):
    """Load from multiple files.

    This is used to read from a temporary directory where
    multiple processes write to concurrently. The files then
    need to be read back and joined together.

    We don't have to care about sorting here as that is done
    in the final write
    """
    full_catalog = {}
    for fpath in dirpath.glob('*.md'):
        partial_catalog = load(fpath)
        full_catalog.update(partial_catalog)
    return full_catalog
