{
  "name": "regex_tokenizer",
  "description": "todo",
  "trainable": false,
  "output_level": "token",
  "outputs": [
    "token"
  ],
  "inputs": [
    "sentence"
  ],
  "type": "tokenizer",
  "file_dependencies": {},
  "spark_input_column_names": [
    "sentence"
  ],
  "spark_output_column_names": [
    "token"
  ],
  "provider": "sparknlp",
  "license": "open source",
  "computation_context": "spark",
  "output_context": "spark"
}