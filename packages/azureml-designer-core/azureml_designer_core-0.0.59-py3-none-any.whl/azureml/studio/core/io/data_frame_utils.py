import os
import pandas as pd
from pyarrow.pandas_compat import _index_level_name
from pandas.core.dtypes.common import is_categorical_dtype, is_bool_dtype
from pandas.core.arrays.integer import _IntegerDtype
import numpy as np

from azureml.studio.core.utils.fileutils import ExecuteInDirectory
from azureml.studio.core.logger import logger
from azureml.studio.core.utils.missing_value_utils import has_na
from azureml.studio.core.schema import ElementTypeName


_PARQUET_ENGINE = 'pyarrow'


def _get_arrow_index_names(df: pd.DataFrame):
    """Get index names which will be generated by pyarrow."""
    n = len(getattr(df.index, 'levels', [df.index]))
    return [_index_level_name(df.index.get_level_values(i), i, df.columns) for i in range(n)]


def _remove_arrow_bug_columns(df):
    # Due to a bug in pyarrow, if the name of a column is the same as the index generated by arrow,
    # an assertion error will be raised when trying reading the parquet file.
    # See https://github.com/apache/arrow/blob/maint-0.12.x/python/pyarrow/pandas_compat.py#L590
    # Repro steps:
    # pd.DataFrame({'__index_level_0__': [1]}).to_parquet(f)
    # pd.read_parquet(f)

    # To avoid such problem, we have to find the invalid columns and drop them before saving parquet files.
    indexes = _get_arrow_index_names(df)
    invalid_columns = [index for index in indexes if index in df.columns]
    if invalid_columns:
        df = df.drop(columns=invalid_columns)
        logger.warning(f"Columns are dropped because their names are invalid to save parquet file. "
                       f"The dropped columns are: {','.join(invalid_columns)}")
    return df


def data_frame_to_parquet(df: pd.DataFrame, parquet_path):
    preserve_index = True
    # If the index is the default index (0, 1, ... n-1) which will be set when initializing a DataFrame,
    # we do not preserve the index to reduce cost and avoid bugs.
    # The operation is O(1) since both indexes are RangeIndex, the operation doesn't introduce much cost.
    if isinstance(df.index, pd.RangeIndex):
        default_index = pd.RangeIndex(0, df.shape[0])
        if df.index.equals(default_index):
            preserve_index = False

    # If we preserve the index, exception will be caused if some column names are invalid due to arrow's bug.
    # We need to remove such columns before writing to parquet.
    if preserve_index:
        df = _remove_arrow_bug_columns(df)

    # In pyarrow==0.16.0, a NotImplementedError will be raised when saving parquet
    # if there is a boolean categorical column in the dataframe.
    # As a workaround, we convert such columns back to normal bool columns then save them.
    # Notice: when the data frame is reloaded back, the categorical information will be lost.
    # By using DataFrameDirectory with schema, the categorical information could be recovered.
    bool_categoricals_cols = {name: df[name] for name in df.columns
                              if is_categorical_dtype(df[name]) and is_bool_dtype(df[name].dtype.categories)
                              }
    for col_name in bool_categoricals_cols:
        df[col_name] = pd.Series(np.asarray(df[col_name]))

    # Use chdir to dest_directory to work around the path issue on windows when writing parquet file
    with ExecuteInDirectory(parquet_path) as parquet_file_name:
        # Use len(df.columns)==0 instead of df.empty because df.empty is true when the df has column names but no row.
        # In this case, the column names should be stored.
        if df is not None and not len(df.columns) == 0:
            if len(set(df.index.map(type))) > 1:
                logger.warning(f'Reset dataframe index from 0 to {df.shape[0]}, since the original index has more than '
                               f'one types, which will cause error when converting to parquet format.')
                df.reset_index(drop=True, inplace=True)

            df.to_parquet(fname=parquet_file_name, engine=_PARQUET_ENGINE,
                          index=preserve_index,
                          # Fix bug 484283: pyarrow raises error due to the precision problem of timestamps.
                          # Add this option so the nanoseconds will be converted to milliseconds
                          allow_truncated_timestamps=True,
                          )

    # Recover the bool categorical columns.
    for col_name, col in bool_categoricals_cols.items():
        df[col_name] = col


def data_frame_from_parquet(parquet_path):
    """
    Read pandas DataFrame from parquet file
    :param parquet_path: str
    :return: pd.DataFrame
    """
    # Use chdir to dest_directory to work around the path issue on windows when writing parquet file
    with ExecuteInDirectory(parquet_path) as parquet_file_name:
        if os.path.exists(parquet_file_name):
            df = pd.read_parquet(parquet_file_name, engine=_PARQUET_ENGINE)
            # Convert column of Int8, Int16, Int32, Int64 and UInt types to float64 if column has missing values,
            # else convert to int64. This is because these types may not be properly handled by some numpy functions.
            for (col_name, column) in df.iteritems():
                if isinstance(column.dtype, _IntegerDtype):
                    if has_na(column):
                        df[col_name] = column.astype(ElementTypeName.FLOAT, errors="ignore")
                        target_type = ElementTypeName.FLOAT
                    else:
                        df[col_name] = column.astype(ElementTypeName.INT, errors="ignore")
                        target_type = ElementTypeName.INT

                    logger.warning(f"Column {col_name} with dtype of {column.dtype} has been converted to \
                        {target_type} dtype, because {column.dtype} type is not fully supported by numpy.")
            return df
        else:
            return pd.DataFrame()


def data_frame_to_csv(df: pd.DataFrame, csv_path, has_header=True):
    df.to_csv(path_or_buf=csv_path, header=has_header, index=False)
