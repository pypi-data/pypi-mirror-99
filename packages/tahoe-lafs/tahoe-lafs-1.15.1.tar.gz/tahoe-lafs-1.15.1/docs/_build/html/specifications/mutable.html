
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Mutable Files &#8212; Tahoe-LAFS 1.x documentation</title>
    <link rel="stylesheet" href="../_static/classic.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/language_data.js"></script>
    
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Tahoe-LAFS Directory Nodes" href="dirnodes.html" />
    <link rel="prev" title="URI Extension Block" href="URI-extension.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="dirnodes.html" title="Tahoe-LAFS Directory Nodes"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="URI-extension.html" title="URI Extension Block"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Tahoe-LAFS 1.x documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="index.html" accesskey="U">Specifications</a> &#187;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="mutable-files">
<h1>Mutable Files<a class="headerlink" href="#mutable-files" title="Permalink to this headline">¶</a></h1>
<ol class="arabic simple">
<li><a class="reference internal" href="#mutable-formats">Mutable Formats</a></li>
<li><a class="reference internal" href="#consistency-vs-availability">Consistency vs. Availability</a></li>
<li><a class="reference internal" href="#the-prime-coordination-directive-don-t-do-that">The Prime Coordination Directive: “Don’t Do That”</a></li>
<li><a class="reference internal" href="#small-distributed-mutable-files">Small Distributed Mutable Files</a><ol class="arabic">
<li><a class="reference internal" href="#sdmf-slots-overview">SDMF slots overview</a></li>
<li><a class="reference internal" href="#server-storage-protocol">Server Storage Protocol</a></li>
<li><a class="reference internal" href="#code-details">Code Details</a></li>
<li><a class="reference internal" href="#smdf-slot-format">SMDF Slot Format</a></li>
<li><a class="reference internal" href="#recovery">Recovery</a></li>
</ol>
</li>
<li><a class="reference internal" href="#medium-distributed-mutable-files">Medium Distributed Mutable Files</a></li>
<li><a class="reference internal" href="#large-distributed-mutable-files">Large Distributed Mutable Files</a></li>
<li><a class="reference internal" href="#todo">TODO</a></li>
</ol>
<p>Mutable files are places with a stable identifier that can hold data that
changes over time. In contrast to immutable slots, for which the
identifier/capability is derived from the contents themselves, the mutable
file identifier remains fixed for the life of the slot, regardless of what
data is placed inside it.</p>
<p>Each mutable file is referenced by two different caps. The “read-write” cap
grants read-write access to its holder, allowing them to put whatever
contents they like into the slot. The “read-only” cap is less powerful, only
granting read access, and not enabling modification of the data. The
read-write cap can be turned into the read-only cap, but not the other way
around.</p>
<p>The data in these files is distributed over a number of servers, using the
same erasure coding that immutable files use, with 3-of-10 being a typical
choice of encoding parameters. The data is encrypted and signed in such a way
that only the holders of the read-write cap will be able to set the contents
of the slot, and only the holders of the read-only cap will be able to read
those contents. Holders of either cap will be able to validate the contents
as being written by someone with the read-write cap. The servers who hold the
shares are not automatically given the ability read or modify them: the worst
they can do is deny service (by deleting or corrupting the shares), or
attempt a rollback attack (which can only succeed with the cooperation of at
least k servers).</p>
<div class="section" id="mutable-formats">
<h2>Mutable Formats<a class="headerlink" href="#mutable-formats" title="Permalink to this headline">¶</a></h2>
<div class="section" id="history">
<h3>History<a class="headerlink" href="#history" title="Permalink to this headline">¶</a></h3>
<p>When mutable files first shipped in Tahoe-0.8.0 (15-Feb-2008), the only
version available was “SDMF”, described below. This was a
limited-functionality placeholder, intended to be replaced with
improved-efficiency “MDMF” files shortly afterwards. The development process
took longer than expected, and MDMF didn’t ship until Tahoe-1.9.0
(31-Oct-2011), and even then it was opt-in (not used by default).</p>
<p>SDMF was intended for relatively small mutable files, up to a few megabytes.
It uses only one segment, so alacrity (the measure of how quickly the first
byte of plaintext is returned to the client) suffers, as the whole file must
be downloaded even if you only want to get a single byte. The memory used by
both clients and servers also scales with the size of the file, instead of
being limited to the half-a-MB-or-so that immutable file operations use, so
large files cause significant memory usage. To discourage the use of SDMF
outside it’s design parameters, the early versions of Tahoe enforced a
maximum size on mutable files (maybe 10MB). Since most directories are built
out of mutable files, this imposed a limit of about 30k entries per
directory. In subsequent releases, this limit was removed, but the
performance problems inherent in the SDMF implementation remained.</p>
<p>In the summer of 2010, Google-Summer-of-Code student Kevan Carstensen took on
the project of finally implementing MDMF. Because of my (Brian) design
mistake in SDMF (not including a separate encryption seed in each segment),
the share format for SDMF could not be used for MDMF, resulting in a larger
gap between the two implementations (my original intention had been to make
SDMF a clean subset of MDMF, where any single-segment MDMF file could be
handled by the old SDMF code). In the fall of 2011, Kevan’s code was finally
integrated, and first made available in the Tahoe-1.9.0 release.</p>
</div>
<div class="section" id="sdmf-vs-mdmf">
<h3>SDMF vs. MDMF<a class="headerlink" href="#sdmf-vs-mdmf" title="Permalink to this headline">¶</a></h3>
<p>The improvement of MDMF is the use of multiple segments: individual 128-KiB
sections of the file can be retrieved or modified independently. The
improvement can be seen when fetching just a portion of the file (using a
Range: header on the webapi), or when modifying a portion (again with a
Range: header). It can also be seen indirectly when fetching the whole file:
the first segment of data should be delivered faster from a large MDMF file
than from an SDMF file, although the overall download will then proceed at
the same rate.</p>
<p>We’ve decided to make it opt-in for now: mutable files default to
SDMF format unless explicitly configured to use MDMF, either in <code class="docutils literal notranslate"><span class="pre">tahoe.cfg</span></code>
(see <a class="reference internal" href="../configuration.html"><span class="doc">Configuring a Tahoe-LAFS node</span></a>) or in the WUI or CLI command that created a
new mutable file.</p>
<p>The code can read and modify existing files of either format without user
intervention. We expect to make MDMF the default in a subsequent release,
perhaps 2.0.</p>
<p>Which format should you use? SDMF works well for files up to a few MB, and
can be handled by older versions (Tahoe-1.8.3 and earlier). If you do not
need to support older clients, want to efficiently work with mutable files,
and have code which will use Range: headers that make partial reads and
writes, then MDMF is for you.</p>
</div>
</div>
<div class="section" id="consistency-vs-availability">
<h2>Consistency vs. Availability<a class="headerlink" href="#consistency-vs-availability" title="Permalink to this headline">¶</a></h2>
<p>There is an age-old battle between consistency and availability. Epic papers
have been written, elaborate proofs have been established, and generations of
theorists have learned that you cannot simultaneously achieve guaranteed
consistency with guaranteed reliability. In addition, the closer to 0 you get
on either axis, the cost and complexity of the design goes up.</p>
<p>Tahoe’s design goals are to largely favor design simplicity, then slightly
favor read availability, over the other criteria.</p>
<p>As we develop more sophisticated mutable slots, the API may expose multiple
read versions to the application layer. The tahoe philosophy is to defer most
consistency recovery logic to the higher layers. Some applications have
effective ways to merge multiple versions, so inconsistency is not
necessarily a problem (i.e. directory nodes can usually merge multiple
“add child” operations).</p>
</div>
<div class="section" id="the-prime-coordination-directive-don-t-do-that">
<h2>The Prime Coordination Directive: “Don’t Do That”<a class="headerlink" href="#the-prime-coordination-directive-don-t-do-that" title="Permalink to this headline">¶</a></h2>
<p>The current rule for applications which run on top of Tahoe is “do not
perform simultaneous uncoordinated writes”. That means you need non-tahoe
means to make sure that two parties are not trying to modify the same mutable
slot at the same time. For example:</p>
<ul class="simple">
<li>don’t give the read-write URI to anyone else. Dirnodes in a private
directory generally satisfy this case, as long as you don’t use two
clients on the same account at the same time</li>
<li>if you give a read-write URI to someone else, stop using it yourself. An
inbox would be a good example of this.</li>
<li>if you give a read-write URI to someone else, call them on the phone
before you write into it</li>
<li>build an automated mechanism to have your agents coordinate writes.
For example, we expect a future release to include a FURL for a
“coordination server” in the dirnodes. The rule can be that you must
contact the coordination server and obtain a lock/lease on the file
before you’re allowed to modify it.</li>
</ul>
<p>If you do not follow this rule, Bad Things will happen. The worst-case Bad
Thing is that the entire file will be lost. A less-bad Bad Thing is that one
or more of the simultaneous writers will lose their changes. An observer of
the file may not see monotonically-increasing changes to the file, i.e. they
may see version 1, then version 2, then 3, then 2 again.</p>
<p>Tahoe takes some amount of care to reduce the badness of these Bad Things.
One way you can help nudge it from the “lose your file” case into the “lose
some changes” case is to reduce the number of competing versions: multiple
versions of the file that different parties are trying to establish as the
one true current contents. Each simultaneous writer counts as a “competing
version”, as does the previous version of the file. If the count “S” of these
competing versions is larger than N/k, then the file runs the risk of being
lost completely. [TODO] If at least one of the writers remains running after
the collision is detected, it will attempt to recover, but if S&gt;(N/k) and all
writers crash after writing a few shares, the file will be lost.</p>
<p>Note that Tahoe uses serialization internally to make sure that a single
Tahoe node will not perform simultaneous modifications to a mutable file. It
accomplishes this by using a weakref cache of the MutableFileNode (so that
there will never be two distinct MutableFileNodes for the same file), and by
forcing all mutable file operations to obtain a per-node lock before they
run. The Prime Coordination Directive therefore applies to inter-node
conflicts, not intra-node ones.</p>
</div>
<div class="section" id="small-distributed-mutable-files">
<h2>Small Distributed Mutable Files<a class="headerlink" href="#small-distributed-mutable-files" title="Permalink to this headline">¶</a></h2>
<p>SDMF slots are suitable for small (&lt;1MB) files that are editing by rewriting
the entire file. The three operations are:</p>
<blockquote>
<div><ul class="simple">
<li>allocate (with initial contents)</li>
<li>set (with new contents)</li>
<li>get (old contents)</li>
</ul>
</div></blockquote>
<p>The first use of SDMF slots will be to hold directories (dirnodes), which map
encrypted child names to rw-URI/ro-URI pairs.</p>
<div class="section" id="sdmf-slots-overview">
<h3>SDMF slots overview<a class="headerlink" href="#sdmf-slots-overview" title="Permalink to this headline">¶</a></h3>
<p>Each SDMF slot is created with a public/private key pair. The public key is
known as the “verification key”, while the private key is called the
“signature key”. The private key is hashed and truncated to 16 bytes to form
the “write key” (an AES symmetric key). The write key is then hashed and
truncated to form the “read key”. The read key is hashed and truncated to
form the 16-byte “storage index” (a unique string used as an index to locate
stored data).</p>
<p>The public key is hashed by itself to form the “verification key hash”.</p>
<p>The write key is hashed a different way to form the “write enabler master”.
For each storage server on which a share is kept, the write enabler master is
concatenated with the server’s nodeid and hashed, and the result is called
the “write enabler” for that particular server. Note that multiple shares of
the same slot stored on the same server will all get the same write enabler,
i.e. the write enabler is associated with the “bucket”, rather than the
individual shares.</p>
<p>The private key is encrypted (using AES in counter mode) by the write key,
and the resulting crypttext is stored on the servers. so it will be
retrievable by anyone who knows the write key. The write key is not used to
encrypt anything else, and the private key never changes, so we do not need
an IV for this purpose.</p>
<p>The actual data is encrypted (using AES in counter mode) with a key derived
by concatenating the readkey with the IV, the hashing the results and
truncating to 16 bytes. The IV is randomly generated each time the slot is
updated, and stored next to the encrypted data.</p>
<p>The read-write URI consists of the write key and the verification key hash.
The read-only URI contains the read key and the verification key hash. The
verify-only URI contains the storage index and the verification key hash.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">URI</span><span class="p">:</span><span class="n">SSK</span><span class="o">-</span><span class="n">RW</span><span class="p">:</span><span class="n">b2a</span><span class="p">(</span><span class="n">writekey</span><span class="p">):</span><span class="n">b2a</span><span class="p">(</span><span class="n">verification_key_hash</span><span class="p">)</span>
<span class="n">URI</span><span class="p">:</span><span class="n">SSK</span><span class="o">-</span><span class="n">RO</span><span class="p">:</span><span class="n">b2a</span><span class="p">(</span><span class="n">readkey</span><span class="p">):</span><span class="n">b2a</span><span class="p">(</span><span class="n">verification_key_hash</span><span class="p">)</span>
<span class="n">URI</span><span class="p">:</span><span class="n">SSK</span><span class="o">-</span><span class="n">Verify</span><span class="p">:</span><span class="n">b2a</span><span class="p">(</span><span class="n">storage_index</span><span class="p">):</span><span class="n">b2a</span><span class="p">(</span><span class="n">verification_key_hash</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that this allows the read-only and verify-only URIs to be derived from
the read-write URI without actually retrieving the public keys. Also note
that it means the read-write agent must validate both the private key and the
public key when they are first fetched. All users validate the public key in
exactly the same way.</p>
<p>The SDMF slot is allocated by sending a request to the storage server with a
desired size, the storage index, and the write enabler for that server’s
nodeid. If granted, the write enabler is stashed inside the slot’s backing
store file. All further write requests must be accompanied by the write
enabler or they will not be honored. The storage server does not share the
write enabler with anyone else.</p>
<p>The SDMF slot structure will be described in more detail below. The important
pieces are:</p>
<ul class="simple">
<li>a sequence number</li>
<li>a root hash “R”</li>
<li>the encoding parameters (including k, N, file size, segment size)</li>
<li>a signed copy of [seqnum,R,encoding_params], using the signature key</li>
<li>the verification key (not encrypted)</li>
<li>the share hash chain (part of a Merkle tree over the share hashes)</li>
<li>the block hash tree (Merkle tree over blocks of share data)</li>
<li>the share data itself (erasure-coding of read-key-encrypted file data)</li>
<li>the signature key, encrypted with the write key</li>
</ul>
<p>The access pattern for read is:</p>
<ul class="simple">
<li>hash read-key to get storage index</li>
<li>use storage index to locate ‘k’ shares with identical ‘R’ values<ul>
<li>either get one share, read ‘k’ from it, then read k-1 shares</li>
<li>or read, say, 5 shares, discover k, either get more or be finished</li>
<li>or copy k into the URIs</li>
</ul>
</li>
<li>read verification key</li>
<li>hash verification key, compare against verification key hash</li>
<li>read seqnum, R, encoding parameters, signature</li>
<li>verify signature against verification key</li>
<li>read share data, compute block-hash Merkle tree and root “r”</li>
<li>read share hash chain (leading from “r” to “R”)</li>
<li>validate share hash chain up to the root “R”</li>
<li>submit share data to erasure decoding</li>
<li>decrypt decoded data with read-key</li>
<li>submit plaintext to application</li>
</ul>
<p>The access pattern for write is:</p>
<ul class="simple">
<li>hash write-key to get read-key, hash read-key to get storage index</li>
<li>use the storage index to locate at least one share</li>
<li>read verification key and encrypted signature key</li>
<li>decrypt signature key using write-key</li>
<li>hash signature key, compare against write-key</li>
<li>hash verification key, compare against verification key hash</li>
<li>encrypt plaintext from application with read-key<ul>
<li>application can encrypt some data with the write-key to make it only
available to writers (use this for transitive read-onlyness of dirnodes)</li>
</ul>
</li>
<li>erasure-code crypttext to form shares</li>
<li>split shares into blocks</li>
<li>compute Merkle tree of blocks, giving root “r” for each share</li>
<li>compute Merkle tree of shares, find root “R” for the file as a whole</li>
<li>create share data structures, one per server:<ul>
<li>use seqnum which is one higher than the old version</li>
<li>share hash chain has log(N) hashes, different for each server</li>
<li>signed data is the same for each server</li>
</ul>
</li>
<li>now we have N shares and need homes for them</li>
<li>walk through peers<ul>
<li>if share is not already present, allocate-and-set</li>
<li>otherwise, try to modify existing share:</li>
<li>send testv_and_writev operation to each one</li>
<li>testv says to accept share if their(seqnum+R) &lt;= our(seqnum+R)</li>
<li>count how many servers wind up with which versions (histogram over R)</li>
<li>keep going until N servers have the same version, or we run out of servers<ul>
<li>if any servers wound up with a different version, report error to
application</li>
<li>if we ran out of servers, initiate recovery process (described below)</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="server-storage-protocol">
<h3>Server Storage Protocol<a class="headerlink" href="#server-storage-protocol" title="Permalink to this headline">¶</a></h3>
<p>The storage servers will provide a mutable slot container which is oblivious
to the details of the data being contained inside it. Each storage index
refers to a “bucket”, and each bucket has one or more shares inside it. (In a
well-provisioned network, each bucket will have only one share). The bucket
is stored as a directory, using the base32-encoded storage index as the
directory name. Each share is stored in a single file, using the share number
as the filename.</p>
<p>The container holds space for a container magic number (for versioning), the
write enabler, the nodeid which accepted the write enabler (used for share
migration, described below), a small number of lease structures, the embedded
data itself, and expansion space for additional lease structures:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>#   offset    size    name
1   0         32      magic verstr &quot;Tahoe mutable container v1\n\x75\x09\x44\x03\x8e&quot;
2   32        20      write enabler&#39;s nodeid
3   52        32      write enabler
4   84        8       data size (actual share data present) (a)
5   92        8       offset of (8) count of extra leases (after data)
6   100       368     four leases, 92 bytes each
                       0    4   ownerid (0 means &quot;no lease here&quot;)
                       4    4   expiration timestamp
                       8   32   renewal token
                       40  32   cancel token
                       72  20   nodeid which accepted the tokens
7   468       (a)     data
8   ??        4       count of extra leases
9   ??        n*92    extra leases
</pre></div>
</div>
<p>The “extra leases” field must be copied and rewritten each time the size of
the enclosed data changes. The hope is that most buckets will have four or
fewer leases and this extra copying will not usually be necessary.</p>
<p>The (4) “data size” field contains the actual number of bytes of data present
in field (7), such that a client request to read beyond 504+(a) will result
in an error. This allows the client to (one day) read relative to the end of
the file. The container size (that is, (8)-(7)) might be larger, especially
if extra size was pre-allocated in anticipation of filling the container with
a lot of data.</p>
<p>The offset in (5) points at the <em>count</em> of extra leases, at (8). The actual
leases (at (9)) begin 4 bytes later. If the container size changes, both (8)
and (9) must be relocated by copying.</p>
<p>The server will honor any write commands that provide the write token and do
not exceed the server-wide storage size limitations. Read and write commands
MUST be restricted to the ‘data’ portion of the container: the implementation
of those commands MUST perform correct bounds-checking to make sure other
portions of the container are inaccessible to the clients.</p>
<p>The two methods provided by the storage server on these “MutableSlot” share
objects are:</p>
<ul class="simple">
<li>readv(ListOf(offset=int, length=int))<ul>
<li>returns a list of bytestrings, of the various requested lengths</li>
<li>offset &lt; 0 is interpreted relative to the end of the data</li>
<li>spans which hit the end of the data will return truncated data</li>
</ul>
</li>
<li>testv_and_writev(write_enabler, test_vector, write_vector)<ul>
<li>this is a test-and-set operation which performs the given tests and only
applies the desired writes if all tests succeed. This is used to detect
simultaneous writers, and to reduce the chance that an update will lose
data recently written by some other party (written after the last time
this slot was read).</li>
<li>test_vector=ListOf(TupleOf(offset, length, opcode, specimen))</li>
<li>the opcode is a string, from the set [gt, ge, eq, le, lt, ne]</li>
<li>each element of the test vector is read from the slot’s data and
compared against the specimen using the desired (in)equality. If all
tests evaluate True, the write is performed</li>
<li>write_vector=ListOf(TupleOf(offset, newdata))<ul>
<li>offset &lt; 0 is not yet defined, it probably means relative to the
end of the data, which probably means append, but we haven’t nailed
it down quite yet</li>
<li>write vectors are executed in order, which specifies the results of
overlapping writes</li>
</ul>
</li>
<li>return value:<ul>
<li>error: OutOfSpace</li>
<li>error: something else (io error, out of memory, whatever)</li>
<li>(True, old_test_data): the write was accepted (test_vector passed)</li>
<li>(False, old_test_data): the write was rejected (test_vector failed)<ul>
<li>both ‘accepted’ and ‘rejected’ return the old data that was used
for the test_vector comparison. This can be used by the client
to detect write collisions, including collisions for which the
desired behavior was to overwrite the old version.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>In addition, the storage server provides several methods to access these
share objects:</p>
<ul class="simple">
<li>allocate_mutable_slot(storage_index, sharenums=SetOf(int))<ul>
<li>returns DictOf(int, MutableSlot)</li>
</ul>
</li>
<li>get_mutable_slot(storage_index)<ul>
<li>returns DictOf(int, MutableSlot)</li>
<li>or raises KeyError</li>
</ul>
</li>
</ul>
<p>We intend to add an interface which allows small slots to allocate-and-write
in a single call, as well as do update or read in a single call. The goal is
to allow a reasonably-sized dirnode to be created (or updated, or read) in
just one round trip (to all N shareholders in parallel).</p>
<div class="section" id="migrating-shares">
<h4>migrating shares<a class="headerlink" href="#migrating-shares" title="Permalink to this headline">¶</a></h4>
<p>If a share must be migrated from one server to another, two values become
invalid: the write enabler (since it was computed for the old server), and
the lease renew/cancel tokens.</p>
<p>Suppose that a slot was first created on nodeA, and was thus initialized with
WE(nodeA) (= H(WEM+nodeA)). Later, for provisioning reasons, the share is
moved from nodeA to nodeB.</p>
<p>Readers may still be able to find the share in its new home, depending upon
how many servers are present in the grid, where the new nodeid lands in the
permuted index for this particular storage index, and how many servers the
reading client is willing to contact.</p>
<p>When a client attempts to write to this migrated share, it will get a “bad
write enabler” error, since the WE it computes for nodeB will not match the
WE(nodeA) that was embedded in the share. When this occurs, the “bad write
enabler” message must include the old nodeid (e.g. nodeA) that was in the
share.</p>
<p>The client then computes H(nodeB+H(WEM+nodeA)), which is the same as
H(nodeB+WE(nodeA)). The client sends this along with the new WE(nodeB), which
is H(WEM+nodeB). Note that the client only sends WE(nodeB) to nodeB, never to
anyone else. Also note that the client does not send a value to nodeB that
would allow the node to impersonate the client to a third node: everything
sent to nodeB will include something specific to nodeB in it.</p>
<p>The server locally computes H(nodeB+WE(nodeA)), using its own node id and the
old write enabler from the share. It compares this against the value supplied
by the client. If they match, this serves as proof that the client was able
to compute the old write enabler. The server then accepts the client’s new
WE(nodeB) and writes it into the container.</p>
<p>This WE-fixup process requires an extra round trip, and requires the error
message to include the old nodeid, but does not require any public key
operations on either client or server.</p>
<p>Migrating the leases will require a similar protocol. This protocol will be
defined concretely at a later date.</p>
</div>
</div>
<div class="section" id="code-details">
<h3>Code Details<a class="headerlink" href="#code-details" title="Permalink to this headline">¶</a></h3>
<p>The MutableFileNode class is used to manipulate mutable files (as opposed to
ImmutableFileNodes). These are initially generated with
client.create_mutable_file(), and later recreated from URIs with
client.create_node_from_uri(). Instances of this class will contain a URI and
a reference to the client (for peer selection and connection).</p>
<p>NOTE: this section is out of date. Please see src/allmydata/interfaces.py
(the section on IMutableFilesystemNode) for more accurate information.</p>
<p>The methods of MutableFileNode are:</p>
<ul class="simple">
<li>download_to_data() -&gt; [deferred] newdata, NotEnoughSharesError<ul>
<li>if there are multiple retrieveable versions in the grid, get() returns
the first version it can reconstruct, and silently ignores the others.
In the future, a more advanced API will signal and provide access to
the multiple heads.</li>
</ul>
</li>
<li>update(newdata) -&gt; OK, UncoordinatedWriteError, NotEnoughSharesError</li>
<li>overwrite(newdata) -&gt; OK, UncoordinatedWriteError, NotEnoughSharesError</li>
</ul>
<p>download_to_data() causes a new retrieval to occur, pulling the current
contents from the grid and returning them to the caller. At the same time,
this call caches information about the current version of the file. This
information will be used in a subsequent call to update(), and if another
change has occured between the two, this information will be out of date,
triggering the UncoordinatedWriteError.</p>
<p>update() is therefore intended to be used just after a download_to_data(), in
the following pattern:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">d</span> <span class="o">=</span> <span class="n">mfn</span><span class="o">.</span><span class="n">download_to_data</span><span class="p">()</span>
<span class="n">d</span><span class="o">.</span><span class="n">addCallback</span><span class="p">(</span><span class="n">apply_delta</span><span class="p">)</span>
<span class="n">d</span><span class="o">.</span><span class="n">addCallback</span><span class="p">(</span><span class="n">mfn</span><span class="o">.</span><span class="n">update</span><span class="p">)</span>
</pre></div>
</div>
<p>If the update() call raises UCW, then the application can simply return an
error to the user (“you violated the Prime Coordination Directive”), and they
can try again later. Alternatively, the application can attempt to retry on
its own. To accomplish this, the app needs to pause, download the new
(post-collision and post-recovery) form of the file, reapply their delta,
then submit the update request again. A randomized pause is necessary to
reduce the chances of colliding a second time with another client that is
doing exactly the same thing:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">d</span> <span class="o">=</span> <span class="n">mfn</span><span class="o">.</span><span class="n">download_to_data</span><span class="p">()</span>
<span class="n">d</span><span class="o">.</span><span class="n">addCallback</span><span class="p">(</span><span class="n">apply_delta</span><span class="p">)</span>
<span class="n">d</span><span class="o">.</span><span class="n">addCallback</span><span class="p">(</span><span class="n">mfn</span><span class="o">.</span><span class="n">update</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_retry</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
  <span class="n">f</span><span class="o">.</span><span class="n">trap</span><span class="p">(</span><span class="n">UncoordinatedWriteError</span><span class="p">)</span>
  <span class="n">d1</span> <span class="o">=</span> <span class="n">pause</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
  <span class="n">d1</span><span class="o">.</span><span class="n">addCallback</span><span class="p">(</span><span class="k">lambda</span> <span class="n">res</span><span class="p">:</span> <span class="n">mfn</span><span class="o">.</span><span class="n">download_to_data</span><span class="p">())</span>
  <span class="n">d1</span><span class="o">.</span><span class="n">addCallback</span><span class="p">(</span><span class="n">apply_delta</span><span class="p">)</span>
  <span class="n">d1</span><span class="o">.</span><span class="n">addCallback</span><span class="p">(</span><span class="n">mfn</span><span class="o">.</span><span class="n">update</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">d1</span>
<span class="n">d</span><span class="o">.</span><span class="n">addErrback</span><span class="p">(</span><span class="n">_retry</span><span class="p">)</span>
</pre></div>
</div>
<p>Enthusiastic applications can retry multiple times, using a randomized
exponential backoff between each. A particularly enthusiastic application can
retry forever, but such apps are encouraged to provide a means to the user of
giving up after a while.</p>
<p>UCW does not mean that the update was not applied, so it is also a good idea
to skip the retry-update step if the delta was already applied:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">d</span> <span class="o">=</span> <span class="n">mfn</span><span class="o">.</span><span class="n">download_to_data</span><span class="p">()</span>
<span class="n">d</span><span class="o">.</span><span class="n">addCallback</span><span class="p">(</span><span class="n">apply_delta</span><span class="p">)</span>
<span class="n">d</span><span class="o">.</span><span class="n">addCallback</span><span class="p">(</span><span class="n">mfn</span><span class="o">.</span><span class="n">update</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_retry</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
  <span class="n">f</span><span class="o">.</span><span class="n">trap</span><span class="p">(</span><span class="n">UncoordinatedWriteError</span><span class="p">)</span>
  <span class="n">d1</span> <span class="o">=</span> <span class="n">pause</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
  <span class="n">d1</span><span class="o">.</span><span class="n">addCallback</span><span class="p">(</span><span class="k">lambda</span> <span class="n">res</span><span class="p">:</span> <span class="n">mfn</span><span class="o">.</span><span class="n">download_to_data</span><span class="p">())</span>
  <span class="k">def</span> <span class="nf">_maybe_apply_delta</span><span class="p">(</span><span class="n">contents</span><span class="p">):</span>
    <span class="n">new_contents</span> <span class="o">=</span> <span class="n">apply_delta</span><span class="p">(</span><span class="n">contents</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">new_contents</span> <span class="o">!=</span> <span class="n">contents</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">mfn</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">new_contents</span><span class="p">)</span>
  <span class="n">d1</span><span class="o">.</span><span class="n">addCallback</span><span class="p">(</span><span class="n">_maybe_apply_delta</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">d1</span>
<span class="n">d</span><span class="o">.</span><span class="n">addErrback</span><span class="p">(</span><span class="n">_retry</span><span class="p">)</span>
</pre></div>
</div>
<p>update() is the right interface to use for delta-application situations, like
directory nodes (in which apply_delta might be adding or removing child
entries from a serialized table).</p>
<p>Note that any uncoordinated write has the potential to lose data. We must do
more analysis to be sure, but it appears that two clients who write to the
same mutable file at the same time (even if both eventually retry) will, with
high probability, result in one client observing UCW and the other silently
losing their changes. It is also possible for both clients to observe UCW.
The moral of the story is that the Prime Coordination Directive is there for
a reason, and that recovery/UCW/retry is not a subsitute for write
coordination.</p>
<p>overwrite() tells the client to ignore this cached version information, and
to unconditionally replace the mutable file’s contents with the new data.
This should not be used in delta application, but rather in situations where
you want to replace the file’s contents with completely unrelated ones. When
raw files are uploaded into a mutable slot through the Tahoe-LAFS web-API
(using POST and the ?mutable=true argument), they are put in place with
overwrite().</p>
<p>The peer-selection and data-structure manipulation (and signing/verification)
steps will be implemented in a separate class in allmydata/mutable.py .</p>
</div>
<div class="section" id="smdf-slot-format">
<h3>SMDF Slot Format<a class="headerlink" href="#smdf-slot-format" title="Permalink to this headline">¶</a></h3>
<p>This SMDF data lives inside a server-side MutableSlot container. The server
is oblivious to this format.</p>
<p>This data is tightly packed. In particular, the share data is defined to run
all the way to the beginning of the encrypted private key (the encprivkey
offset is used both to terminate the share data and to begin the encprivkey).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span> #    offset   size    name
 1    0        1       version byte, \x00 for this format
 2    1        8       sequence number. 2^64-1 must be handled specially, TBD
 3    9        32      &quot;R&quot; (root of share hash Merkle tree)
 4    41       16      IV (share data is AES(H(readkey+IV)) )
 5    57       18      encoding parameters:
       57       1        k
       58       1        N
       59       8        segment size
       67       8        data length (of original plaintext)
 6    75       32      offset table:
       75       4        (8) signature
       79       4        (9) share hash chain
       83       4        (10) block hash tree
       87       4        (11) share data
       91       8        (12) encrypted private key
       99       8        (13) EOF
 7    107      436ish  verification key (2048 RSA key)
 8    543ish   256ish  signature=RSAsign(sigkey, H(version+seqnum+r+IV+encparm))
 9    799ish   (a)     share hash chain, encoded as:
                        &quot;&quot;.join([pack(&quot;&gt;H32s&quot;, shnum, hash)
                                 for (shnum,hash) in needed_hashes])
10    (927ish) (b)     block hash tree, encoded as:
                        &quot;&quot;.join([pack(&quot;&gt;32s&quot;,hash) for hash in block_hash_tree])
11    (935ish) LEN     share data (no gap between this and encprivkey)
12    ??       1216ish encrypted private key= AESenc(write-key, RSA-key)
13    ??       --      EOF

(a) The share hash chain contains ceil(log(N)) hashes, each 32 bytes long.
   This is the set of hashes necessary to validate this share&#39;s leaf in the
   share Merkle tree. For N=10, this is 4 hashes, i.e. 128 bytes.
(b) The block hash tree contains ceil(length/segsize) hashes, each 32 bytes
   long. This is the set of hashes necessary to validate any given block of
   share data up to the per-share root &quot;r&quot;. Each &quot;r&quot; is a leaf of the share
   has tree (with root &quot;R&quot;), from which a minimal subset of hashes is put in
   the share hash chain in (8).
</pre></div>
</div>
</div>
<div class="section" id="recovery">
<h3>Recovery<a class="headerlink" href="#recovery" title="Permalink to this headline">¶</a></h3>
<p>The first line of defense against damage caused by colliding writes is the
Prime Coordination Directive: “Don’t Do That”.</p>
<p>The second line of defense is to keep “S” (the number of competing versions)
lower than N/k. If this holds true, at least one competing version will have
k shares and thus be recoverable. Note that server unavailability counts
against us here: the old version stored on the unavailable server must be
included in the value of S.</p>
<p>The third line of defense is our use of testv_and_writev() (described below),
which increases the convergence of simultaneous writes: one of the writers
will be favored (the one with the highest “R”), and that version is more
likely to be accepted than the others. This defense is least effective in the
pathological situation where S simultaneous writers are active, the one with
the lowest “R” writes to N-k+1 of the shares and then dies, then the one with
the next-lowest “R” writes to N-2k+1 of the shares and dies, etc, until the
one with the highest “R” writes to k-1 shares and dies. Any other sequencing
will allow the highest “R” to write to at least k shares and establish a new
revision.</p>
<p>The fourth line of defense is the fact that each client keeps writing until
at least one version has N shares. This uses additional servers, if
necessary, to make sure that either the client’s version or some
newer/overriding version is highly available.</p>
<p>The fifth line of defense is the recovery algorithm, which seeks to make sure
that at least <em>one</em> version is highly available, even if that version is
somebody else’s.</p>
<p>The write-shares-to-peers algorithm is as follows:</p>
<ul class="simple">
<li>permute peers according to storage index</li>
<li>walk through peers, trying to assign one share per peer</li>
<li>for each peer:<ul>
<li>send testv_and_writev, using “old(seqnum+R) &lt;= our(seqnum+R)” as the test<ul>
<li>this means that we will overwrite any old versions, and we will
overwrite simultaenous writers of the same version if our R is higher.
We will not overwrite writers using a higher seqnum.</li>
</ul>
</li>
<li>record the version that each share winds up with. If the write was
accepted, this is our own version. If it was rejected, read the
old_test_data to find out what version was retained.</li>
<li>if old_test_data indicates the seqnum was equal or greater than our
own, mark the “Simultanous Writes Detected” flag, which will eventually
result in an error being reported to the writer (in their close() call).</li>
<li>build a histogram of “R” values</li>
<li>repeat until the histogram indicate that some version (possibly ours)
has N shares. Use new servers if necessary.</li>
<li>If we run out of servers:<ul>
<li>if there are at least shares-of-happiness of any one version, we’re
happy, so return. (the close() might still get an error)</li>
<li>not happy, need to reinforce something, goto RECOVERY</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Recovery:</p>
<ul class="simple">
<li>read all shares, count the versions, identify the recoverable ones,
discard the unrecoverable ones.</li>
<li>sort versions: locate max(seqnums), put all versions with that seqnum
in the list, sort by number of outstanding shares. Then put our own
version. (TODO: put versions with seqnum &lt;max but &gt;us ahead of us?).</li>
<li>for each version:<ul>
<li>attempt to recover that version</li>
<li>if not possible, remove it from the list, go to next one</li>
<li>if recovered, start at beginning of peer list, push that version,
continue until N shares are placed</li>
<li>if pushing our own version, bump up the seqnum to one higher than
the max seqnum we saw</li>
<li>if we run out of servers:<ul>
<li>schedule retry and exponential backoff to repeat RECOVERY</li>
</ul>
</li>
<li>admit defeat after some period? presumeably the client will be shut down
eventually, maybe keep trying (once per hour?) until then.</li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="medium-distributed-mutable-files">
<h2>Medium Distributed Mutable Files<a class="headerlink" href="#medium-distributed-mutable-files" title="Permalink to this headline">¶</a></h2>
<p>These are just like the SDMF case, but:</p>
<ul class="simple">
<li>We actually take advantage of the Merkle hash tree over the blocks, by
reading a single segment of data at a time (and its necessary hashes), to
reduce the read-time alacrity.</li>
<li>We allow arbitrary writes to any range of the file.</li>
<li>We add more code to first read each segment that a write must modify.
This looks exactly like the way a normal filesystem uses a block device,
or how a CPU must perform a cache-line fill before modifying a single word.</li>
<li>We might implement some sort of copy-based atomic update server call,
to allow multiple writev() calls to appear atomic to any readers.</li>
</ul>
<p>MDMF slots provide fairly efficient in-place edits of very large files (a few
GB). Appending data is also fairly efficient.</p>
</div>
<div class="section" id="large-distributed-mutable-files">
<h2>Large Distributed Mutable Files<a class="headerlink" href="#large-distributed-mutable-files" title="Permalink to this headline">¶</a></h2>
<p>LDMF slots (not implemented) would use a fundamentally different way to store
the file, inspired by Mercurial’s “revlog” format. This would enable very
efficient insert/remove/replace editing of arbitrary spans. Multiple versions
of the file can be retained, in a revision graph that can have multiple heads.
Each revision can be referenced by a cryptographic identifier. There are two
forms of the URI, one that means “most recent version”, and a longer one that
points to a specific revision.</p>
<p>Metadata can be attached to the revisions, like timestamps, to enable rolling
back an entire tree to a specific point in history.</p>
<p>LDMF1 provides deltas but tries to avoid dealing with multiple heads. LDMF2
provides explicit support for revision identifiers and branching.</p>
</div>
<div class="section" id="todo">
<h2>TODO<a class="headerlink" href="#todo" title="Permalink to this headline">¶</a></h2>
<p>improve allocate-and-write or get-writer-buckets API to allow one-call (or
maybe two-call) updates. The challenge is in figuring out which shares are on
which machines. First cut will have lots of round trips.</p>
<p>(eventually) define behavior when seqnum wraps. At the very least make sure
it can’t cause a security problem. “the slot is worn out” is acceptable.</p>
<p>(eventually) define share-migration lease update protocol. Including the
nodeid who accepted the lease is useful, we can use the same protocol as we
do for updating the write enabler. However we need to know which lease to
update.. maybe send back a list of all old nodeids that we find, then try all
of them when we accept the update?</p>
<dl class="docutils">
<dt>We now do this in a specially-formatted IndexError exception:</dt>
<dd>“UNABLE to renew non-existent lease. I have leases accepted by ” +
“nodeids: ‘12345’,’abcde’,‘44221’ .”</dd>
</dl>
<p>confirm that a repairer can regenerate shares without the private key. Hmm,
without the write-enabler they won’t be able to write those shares to the
servers.. although they could add immutable new shares to new servers.</p>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../index.html">Table of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Mutable Files</a><ul>
<li><a class="reference internal" href="#mutable-formats">Mutable Formats</a><ul>
<li><a class="reference internal" href="#history">History</a></li>
<li><a class="reference internal" href="#sdmf-vs-mdmf">SDMF vs. MDMF</a></li>
</ul>
</li>
<li><a class="reference internal" href="#consistency-vs-availability">Consistency vs. Availability</a></li>
<li><a class="reference internal" href="#the-prime-coordination-directive-don-t-do-that">The Prime Coordination Directive: “Don’t Do That”</a></li>
<li><a class="reference internal" href="#small-distributed-mutable-files">Small Distributed Mutable Files</a><ul>
<li><a class="reference internal" href="#sdmf-slots-overview">SDMF slots overview</a></li>
<li><a class="reference internal" href="#server-storage-protocol">Server Storage Protocol</a><ul>
<li><a class="reference internal" href="#migrating-shares">migrating shares</a></li>
</ul>
</li>
<li><a class="reference internal" href="#code-details">Code Details</a></li>
<li><a class="reference internal" href="#smdf-slot-format">SMDF Slot Format</a></li>
<li><a class="reference internal" href="#recovery">Recovery</a></li>
</ul>
</li>
<li><a class="reference internal" href="#medium-distributed-mutable-files">Medium Distributed Mutable Files</a></li>
<li><a class="reference internal" href="#large-distributed-mutable-files">Large Distributed Mutable Files</a></li>
<li><a class="reference internal" href="#todo">TODO</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="URI-extension.html"
                        title="previous chapter">URI Extension Block</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="dirnodes.html"
                        title="next chapter">Tahoe-LAFS Directory Nodes</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/specifications/mutable.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="dirnodes.html" title="Tahoe-LAFS Directory Nodes"
             >next</a> |</li>
        <li class="right" >
          <a href="URI-extension.html" title="URI Extension Block"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Tahoe-LAFS 1.x documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="index.html" >Specifications</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2016, The Tahoe-LAFS Developers.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.8.5.
    </div>
  </body>
</html>