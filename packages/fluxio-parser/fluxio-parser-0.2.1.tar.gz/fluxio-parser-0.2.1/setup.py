# -*- coding: utf-8 -*-
from setuptools import setup

packages = \
['fluxio_parser',
 'fluxio_parser.states',
 'fluxio_parser.states.tasks',
 'fluxio_parser.transformers',
 'fluxio_parser.visitors']

package_data = \
{'': ['*']}

install_requires = \
['astor>=0.8.1,<0.9.0', 'black>=19.3b0,<20.0', 'networkx>=2.5,<3.0']

setup_kwargs = {
    'name': 'fluxio-parser',
    'version': '0.2.1',
    'description': 'Fluxio parser library',
    'long_description': '# Fluxio Parser\n\nFluxio is a framework for building workflows using Python. This is the parser component. Its job is to parse a Fluxio project\'s Python DSL into an in-memory representation. Other components translate the parsed project into deployable artifacts.\n\n- [Fluxio DSL](#fluxio-dsl)\n- [Input Data](#input-data)\n- [States](#states)\n- [Task](#task)\n  - [Definition](#definition)\n  - [Adding to the state machine](#adding-to-the-state-machine)\n  - [Passing data to tasks](#passing-data-to-tasks)\n  - [Stopping the execution](#stopping-the-execution)\n  - [Working with the State Data Client](#working-with-the-state-data-client)\n  - [Error Handling](#error-handling)\n  - [Retries](#retries)\n  - [Choice](#choice)\n  - [Map](#map)\n  - [Parallel](#parallel)\n  - [Pass](#pass)\n  - [Succeed](#succeed)\n  - [Fail](#fail)\n  - [Wait](#wait)\n- [Decorators](#decorators)\n  - [schedule](#schedule)\n  - [subscribe](#subscribe)\n  - [export](#export)\n\n## Fluxio DSL\n\nFluxio employs a [DSL](https://en.wikipedia.org/wiki/Domain-specific_language) written in Python syntax; this means a file\'s [abstract syntax tree (AST)](https://docs.python.org/3/library/ast.html) is parsed from source code instead of the module being executed directly by the Python interpreter.\n\nAn Fluxio project file contains:\n\n- A module-level function named "main" that defines the state machine logic. This function will be parsed later transpiled to [Amazon States Language (ASL)](https://docs.aws.amazon.com/step-functions/latest/dg/concepts-amazon-states-language.html).\n  - The function should define a single positional arg, data, for explicitness but technically it doesn\'t matter. This variable represents the input data to the state machine execution, referenced as $ in ASL.\n  - See the [States](#states) section below for how to define the various states in Fluxio syntax\n- If the state machine needs any task states, then one or more module-level classes should be defined.\n  - Each class must have a unique, PascalCased name.\n    - Each class must inherit from [`Task`](#task).\n    - Each class should define a `run` method that takes the following positional arguments:\n      - `event` will be the input data\n      - `context` contains clients, functions, and attributes related to task metadata\n\n## [Input Data](https://docs.aws.amazon.com/step-functions/latest/dg/concepts-input-output-filtering.html)\n\nWithin the main function, the data variable is the state input and is referenced as $ in ASL. We used it to pass parameters into and store data resulting from Task states. You can also set static values or transform the input data with a Pass state. The data variable is a dictionary.\n\n## States\n\nThe `states` subpackage has modules that roughly correspond to the actual state machine states. There is a base class called `StateMachineFragment` that represents some chunk of the state machine. The base `State` class only really exists to provide a more conceptually readable parent to the various subclasses in the states subpackage. State machine fragments that are not states include `ChoiceBranch`, parallel\'s `Branch`, and task\'s `Catch`.\n\nThe `tasks` subpackage within `states` contains different types of task states that resolve differently depending on the service specified in the Fluxio file. We have a task state that works with sync Lambda Functions and sync ECS tasks. A factory function in the subpackage\'s `__init__.py` determines the relevant task state class.\n\nThe following subsections explain how to represent a given ASL State in a Fluxio file. Click on each section heading to learn about each state\'s purpose.\n\n## [Task](https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-task-state.html)\n\n### Definition\n\nTo define a task, add a Task subclass with a run method:\n\n```python\nclass Bar(Task):\n    async def run(event, context):\n        event.update({"bar": 456})\n        return event\n```\n\n#### `run` method\n\nThe run method should be async for consistency. The Lambda/ECS entry point code will get the current event loop and run the method.\n\nThe run method will be extracted **as-is** and used to replace a module in the generated Python package before the package is built for deployment. This means any import statements should go in the body of this method. You can include any application code you want. However, if your run method is more than ~50 LOC, you should probably create a separate library package then import and execute it in the run method.\n\nThe `ecs:worker` service type does not use the `run` method.\n\n#### `service` attribute\n\nBy default, the task will be deployed as a Lambda Function. To explicitly set the service (the runtime of the task), add a class attribute:\n\n```python\nclass Bar(Task):\n    service = "ecs"\n    async def run(event, context):\n        from ns_ml_runtime_thing import do_ml\n\n        do_ml(event)\n```\n\nOptions currently include:\n\n- `lambda`\n- `ecs`\n- `lambda:pexpm-runner`\n  - __NOTE:__ PEXPM Runner is a Lambda function that downloads a PEX binary to /tmp and executes it in a subprocess at runtime. This should only be used to get around the 250MB artifact limit.\n- `ecs:worker`\n  - See below for specifics\n\n##### ECS Worker\n\nThe `ecs:worker` service type uses the ["Wait for Task Token" service integration pattern in Step Functions](https://docs.aws.amazon.com/step-functions/latest/dg/connect-to-resource.html#connect-wait-token). This means instead of directly running a task, like a Lambda Function or ECS task, a message is sent to an SQS queue for processing by an external system. In Fluxio, the external system is an ECS Fargate service. The tasks in the service are queue workers; they poll the SQS queue and execute business logic based on the message. All the SQS and ECS infrastructure is managed by Fluxio (via CloudFormation) just like other service types.\n\nThis service type is a good fit if your use case:\n\n- Needs to use ECS: maybe your package artifact size is too big for Lambda or you need to run a task for longer than 15 minutes\n- Exceeds the [maximum number of tasks that can be launched per RunTask API action](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-quotas.html)\n\nThe ECS worker pattern allows to run one or more workers in the background to support long-running tasks as well as limit the number of API requests that Step Functions makes to ECS.\n\nTo get started, first extend the `TaskWorker` class and put your code in a regular package. For example, we\'ll define a new class called `TestWorker` in the `ns_worker_test` package in the `worker.py` module:\n\n```python\n"""Contains TestWorker class"""\n\nimport logging\nfrom typing import Dict\n\nimport aioredis\n\nfrom ns_sfn_task_entry_points.ecs_worker_app import TaskContext, TaskWorker\n\nlogger = logging.getLogger()\n\n\nclass TestWorker(TaskWorker):\n    """Test worker"""\n\n    async def on_startup(self):\n        """Initialize global application state"""\n        await super().on_startup()\n        self.cache_client_engine = await aioredis.create_redis_pool(...)\n\n    async def on_cleanup(self) -> None:\n        """Tear down the worker context"""\n        await super().on_cleanup()\n        self.cache_client_engine.close()\n        await self.cache_client_engine.wait_closed()\n\n    async def run(self, event: Dict, context: TaskContext):\n        """Run the task, i.e. handle a single queue message\n\n        This method exists for compatibility with other Fluxio tasks.\n\n        Args:\n            event: Event/input data unpacked from the queue message\n            context: Task context object containing clients, functions, and metadata\n\n        """\n        item = context.state_data_client.get_item_for_map_iteration(event)\n        logger.info(item)\n```\n\nNote that you can define `on_startup` and `on_cleanup` lifecycle methods. These allow you to create database engines and API clients once when the application launches instead of with every message.\n\nNext, define a Task in your `project.py` file and at least the `spec` attribute:\n\n```python\nclass GenerateItems(Task):\n    async def run(event, context):\n        return context.state_data_client.put_items(\n            "items",\n            [{"name": "sue"}, {"name": "jae"}, {"name": "levi"}]\n        )\n\nclass Worker(Task):\n    service = "ecs:worker"\n    spec = "ns_worker_test.worker:TestWorker"\n    timeout = 600\n    concurrency = 10\n    heartbeat_interval = 60\n    autoscaling_min = 1\n    autoscaling_max = 2\n\ndef process_item(data):\n    Worker()\n\ndef main(data):\n    data["items"] = GenerateItems()\n    map(data["items"], process_item)\n```\n\nAvailable attributes:\n\n- `spec`: reference the path to your new class in the format `package.module:Class`\n- `concurrency` (default: 1): maximum number of messages to concurrently process within each task. Value must be in range 1-100. If the message handler does CPU-intensive work, this should be set to 1. The memory/CPU allotted to the task will determine how high this number can go. For I/O-intensive work, this number can generally be set to 10 per GB of memory but your mileage may vary.\n- `heartbeat_interval` (default: None): interval in seconds between heartbeat events sent to SQS. This value must be below the task timeout value. If the value is None, the task will not send heartbeats and the message timeout will default to the queue\'s timeout. A heartbeat "resets the clock" on an individual message\'s visibility timeout. Once a heartbeat happens, then the message will become visible in `<interval> * 2` seconds unless another heartbeat occurs in `<interval>` seconds. If the task stops (like during a deployment), the timeout will expire and another task can receive the message. See the docs at [Amazon SQS visibility timeout\n](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html) for more details.\n- `autoscaling_min` (default: 1): minimum number of ECS tasks. Setting this to 1 means the service will always run at least one worker\n- `autoscaling_max` (default: 1): maximum number of ECS tasks\n\n#### `timeout` attribute\n\nBy default the task will timeout after 300 seconds. To change that value, set a class attribute:\n\n```python\nclass Bar(Task):\n    timeout = 600  # 10 minutes\n    async def run(event, context):\n        # ...\n```\n\nYou also need to provide that new timeout value as a keyword argument when you use the task:\n\n```python\ndef main(data):\n    Bar(key="Bar", timeout=600)\n```\n\n#### `cpu` and `memory` attributes\n\nThe `memory` attribute is only used by Lambda and ECS tasks. The `cpu` attribute is only used by ECS tasks.\n\nBy default the `cpu` is set to 1024 and the `memory` is set to 2048. To change those values, set class attributes:\n\n```python\nclass Bar(Task):\n    cpu = 2048  # 2 vCPU\n    memory = 4096  # 4 GB\n    async def run(event, context):\n        # ...\n```\n\nFor ECS:\n\n- The available CPU values are 256, 512, 1024, 2048, 4096\n- The memory value is tied to the CPU but should generally be set to at least 2 times greater than the CPU value\n\nFor Lambda:\n\n- The available memory values are 128, 512, 1024, 1536, 2048, 3008\n\n### Adding to the state machine\n\nTo add a Task state, instantiate the Task class within the main function. You can either:\n\n- Instantiate the class and assign the result to a key in the input data (recommended).\n  - This is supported for services that can return a result from the task. Only Lambda can do this. This means that returning a result in the run method will not do anything if the service is set to "ecs".\n- Instantiate the class and do not assign its result.\n  - This means that the result of the task will be discarded, i.e. it won\'t show up on the input data object and therefore won\'t be available to downstream states.\n\nTo set an explicit key for the task state (recommended), pass a value for the key keyword argument. Otherwise, the key in the States dictionary will be generated automatically.\n\n```python\ndata["foo_result"] = Foo(key="Do a foo")  # this will update the input data\nFoo(key="Do a foo")  # this will not update the input data\n```\n\n### Passing data to tasks\n\nBy default, the input path to a task is the full data dict ($). If you want to pass part of the data to a task, provide a positional argument to the task constructor.\n\n```python\ndata["foo_result"] = Foo(key="Do a foo")\ndata["bar_result"] = Bar(data["foo_result"], key="Do the bar")\n```\n\n### Stopping the execution\n\nIf you need to stop/cancel/abort the execution from within a task, you can use the  `context.stop_execution` method within your task\'s `run` method. A common use case is if you need to check the value of a feature flag at the beginning of the execution and abort if it\'s false. For example:\n\n```python\nif not some_condition:\n    return await context.stop_execution()\n```\n\nYou can provide extra detail by passing `error` and `cause` keyword arguments to the `stop_execution` method. The `error` is a short string like a code or enum value whereas `cause` is a longer description.\n\n### Working with the State Data Client\n\nOne of the stated Step Functions best practices is to avoid passing large payloads between states; the input data limit is only 32K characters. To get around this, you can choose to store data from your task code in a DynamoDB table. With DynamoDB, we have an item limit of 400KB to work with. When you put items into the table you receive a pointer to the DynamoDB item which you can return from your task so it gets includes in the input data object. From there, since the pointer is in the `data` dict, you can reload the stored data in a downstream task. There is a library, `ns_sfn_tools`, with a State Data Client instance for putting and getting items from this DynamoDB table. It\'s available in your task\'s `run` method as `context.state_data_client`.\n\nThe client methods are split between "local" and "global" variants. Local methods operate on items stored within the project whereas global methods can operate on items that were stored from any project. Global methods require a fully-specified partition key (primary key, contains the execution ID) and table name to locate the item whereas local methods only need a simple key because the partition key and table name can be infered from the project automatically. The `put_*` methods return a dict with metadata about the location of the item, including the `key`, `partition_key`, and `table_name`. If you return this metadata object from a task, it will get put on the `data` object and you can call a `get_*` method later in the state machine.\n\nMany methods also accept an optional `index` argument. This argument needs to be provided when getting/putting an item that was originally stored as part of a `put_items` or `put_global_items` call. Providing the `index` is usually only done within a map iteration task.\n\nBelow are a few of the more common methods:\n\n#### `put_item`/`put_items`\n\nThe `put_item` method puts an item in the state store. It takes `key`, `data`, and `index` arguments. For example:\n\n```python\ncontext.state_data_client.put_item("characters", {"name": "jerry"})\ncontext.state_data_client.put_item("characters", {"name": "elaine"}, index=24)\n```\n\nNote that the item at the given array index doesn\'t actually have to exist in the table before you call `put_item`. However, if it doesn\'t exist then you may have a fan-out logic bug upstream in your state machine.\n\nThe `put_items` method puts an entire list of items into the state store. Each item will be stored separately under its corresponding array index. For example:\n\n```python\ncontext.state_data_client.put_items("characters", [{"name": "jerry"}, {"name": "elaine"}])\n```\n\n#### `get_item`\n\nThe `get_item` method gets the data attribute from an item in the state store. It takes `key` and `index` arguments. For example:\n\n```python\ncontext.state_data_client.get_item("characters")  # -> {"name": "jerry"}\ncontext.state_data_client.get_item("characters", index=24)  # -> {"name": "elaine"}\n```\n\n#### `get_item_for_map_iteration`/`get_global_item_for_map_iteration`\n\nThe `get_item_for_map_iteration` method gets the data attribute from an item in the state store using the `event` object. This method only works when called within a map iterator task. For example, if the `put_items` example above was called in a task, and its value was given to a map state to fan out, we can use the `get_item_for_map_iteration` method within our iterator task to fetch each item:\n\n```python\n# Iteration 0:\ncontext.state_data_client.get_item_for_map_iteration(event)  # -> {"name": "jerry"}\n# Iteration 1:\ncontext.state_data_client.get_item_for_map_iteration(event)  # -> {"name": "elaine"}\n```\n\nThis works because the map iterator state machine receives an input data object with the schema:\n\n```json\n{\n  "items_result_table_name": "<DynamoDB table for the project>",\n  "items_result_partition_key": "<execution ID>:characters",\n  "items_result_key": "characters",\n  "context_index": "<array index>",\n  "context_value.$": "1"\n}\n```\n\nThe `get_item_for_map_iteration` is a helper method that uses that input to locate the right item. The `get_global_item_for_map_iteration` method has the same signature. It should be called when you know that the array used to fan out could have come from another project (e.g. the map state is the first state in a state machine triggered by a subscription).\n\n### Error Handling\n\nTo handle an error in the task state, wrap it in a try/except statement. This will translate to an array of Catch objects within the rendered Task state.\n\n```python\ntry:\n    Foo()\nexcept (KeyError, States.Timeout):\n    TaskWhenFooHasErrored()\nexcept:\n    GenericTask()\n```\n\n```python\n{\n    "Type": "Task",\n    "Resource": "...",\n    "Catch": [\n        {\n            "Next": "foo_fail",\n            "ErrorEquals": ["KeyError", "States.Timeout"],\n        },\n        {"Next": "foo_general", "ErrorEquals": ["States.ALL"]},\n    ]\n}\n```\n\n### Retries\n\nTo retry a task when it fails, use the retry context manager:\n\n```python\nwith retry():\n    MyTask()\n```\n\nYou can configure the retry behavior by passing keyword arguments:\n\n- `on_exceptions`: A list of Exception classes that will trigger another attempt (all exceptions by default)\n- `interval`: An integer that represents the number of seconds before the first retry attempt (1 by default)\n- `max_attempts`: A positive integer that represents the maximum number of retry attempts (3 by default)\n- `backoff_rate`: The multiplier by which the retry interval increases during each attempt (2.0 by default)\n\nFor example:\n\n```python\nwith retry(\n    on_exceptions=[CustomError],\n    interval=10,\n    max_attempts=5,\n    backoff_rate=1.0\n):\n    MyTask()\n```\n\nThe retry context manager can only wrap a single task. If you want to also include error handling, the try statement should have the retry context manager as the one and only item in its body. For example:\n\n```python\ntry:\n    with retry():\n        Foo()\nexcept:\n    GenericTask()\n```\n\n### [Choice](https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-choice-state.html)\n\nTo conditionally choose which logical path to traverse next in the state machine you can use Python boolean expressions.\n\nSince the ASL form of the Choice state requires type-specific keys like StringEquals and NumericLessThan, but Python is an untyped language, we need a way to figure out the data types of the operands within conditional statements. One approach is to explicitly cast references to the data variable with the built-in function str, int, float, or bool. This enables Fluxio to generate type-appropriate configuration. If a reference to data isn\'t wrapped, Fluxio will assume it\'s a string for the boolean expression.\n\nHowever, most of the time you don\'t need to explicitly wrap data values; Fluxio will automatically infer types from static, scalar values. This means that if you\'re comparing a value from data and a scalar value, Fluxio will use the scalar value type to pick the right ASL configuration. For example, for the comparison `data["foo"] > 0`, we know that 0 is a number and will pick the NumericGreaterThan operation.\n\nWithin the body of the conditional, you can include state code just like the main function. This will be translated to the Next key of the Choice branch.\n\nThe body of the else clause is translated to the Default key in the configuration.\n\n```python\nif data["foo"] > 0 and data["foo"] < 100:\n    raise Bad("nope")\nelif not bool(data["empty"]):\n    return\nelif data["empty"]["inner"] == "something":\n    return\nelif data["empty"]["inner"] == 4.25:\n    parallel(branch1, branch2)\n    if data["done"] != 10:\n        raise DoneButNot()\nelif data["array"][5] == 5:\n    Bar()\n    Baz()\nelse:\n    raise Wrong("mmk")\n```\n\n```python\n{\n    "Type": "Choice",\n    "Choices": [\n        {\n            "Next": "Fail-...",\n            "And": [\n                {"Variable": "$[\'foo\']", "NumericGreaterThan": 0},\n                {"Variable": "$[\'foo\']", "NumericLessThan": 100},\n            ],\n        },\n        {\n            "Next": "Success-...",\n            "Not": {\n                "Variable": "$[\'empty\']",\n                "BooleanEquals": True,\n            },\n        },\n        {\n            "Next": "Success-...",\n            "Variable": "$[\'empty\'][\'inner\']",\n            "StringEquals": "something",\n        },\n        {\n            "Next": "Parallel-...",\n            "Variable": "$[\'empty\'][\'inner\']",\n            "NumericEquals": 4.25,\n        },\n        {\n            "Next": "bar2",\n            "Variable": "$[\'array\'][5]",\n            "NumericEquals": 5,\n        },\n    ],\n    "Default": "Fail-..."\n}\n```\n\n**Note:** Timestamp comparison operators are not currently supported.\n\n### [Map](https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-map-state.html)\n\nTo execute a dynamic number of nested state machines in parallel, you can use the Map state. First you need to define a module-level function that contains state machine logic in the body of the function just as you would in `main`. The function names are arbitrary as long as they\'re unique.\n\nThen, within the main function, call the map function, passing a reference to an array in data and the iterator function as positional arguments:\n\n```python\ndef item_iterator(data):\n    Baz()\n\ndef main(data):\n    map(data["items"], item_iterator)\n```\n\nIf you want to limit the number of concurrently running items, provide a max_concurrency keyword arg:\n\n```python\nmap(data["items"], item_iterator, max_concurrency=3)\n```\n\n### [Parallel](https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-parallel-state.html)\n\nTo execute multiple branches in parallel, you first need to define the branch states. Add a module-level function and include state machine logic in the body of the function just as you would in a main function. The function names are arbitrary as long as they\'re unique.\n\nThen, within the main function, call the `parallel` function and pass it the branch function references as positional arguments:\n\n```python\ndef branch1():\n    Baz()\n\ndef branch2():\n    Foo()\n\ndef main(data):\n    parallel(branch1, Task2)\n```\n\nNote that the number of branches must be defined statically. If you need dynamic fan-out, use the Map state.\n\n### [Pass](https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-pass-state.html)\n\nUse this state to set static keys on the data variable. You can use subscript notation or the update method:\n\n```python\n# Set the key "debug" on the input data dict to be a static dictionary\ndata["debug"] = {"level": "INFO"}\n# Set the key "more" on the input data dict to equal a static value\ndata.update({"more": 123})\n\n# Not currently supported:\ndata["debug"].update({"level": "INFO"})\n```\n\n```python\n{\n    "Type": "Pass",\n    "Result": {"level": "INFO"},\n    "ResultPath": "$[\'debug\']"\n}\n{\n    "Type": "Pass",\n    "Result": {"more": 123},\n    "ResultPath": "$"\n}\n```\n\n### [Succeed](https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-succeed-state.html)\n\nTo end the state machine execution and indicate a successful completion, include the return keyword within the main function.\n\n### [Fail](https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-fail-state.html)\n\nTo end the state machine execution and indicate a failure, raise an exception. The exception class name will be used as the Error key and the optional string you pass to the exception constructor will b used as the Cause key.\n\n```python\nraise Wrong("mmk")\n```\n\n```python\n{\n    "Type": "Fail",\n    "Error": "Wrong",\n    "Cause": "mmk"\n}\n```\n\n### [Wait](https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-wait-state.html)\n\nIf you want to pause the execution of the state machine, you can call a function called wait with either a seconds or timestamp keyword argument. The value of the argument can be a static value or input data reference:\n\n```python\n# Wait 10 seconds\nwait(seconds=10)\nwait(seconds=data["wait_in_seconds"])\n\n# Wait until a future time\nwait(timestamp="2020-08-18T17:33:00Z")\nwait(timestamp=data["timestamp"])\n```\n\n## Decorators\n\nFluxio supports additional configuration of the state machine via Python decorators. These are meant to configure pre- and post-execution hooks (i.e. something "outside" the execution), like a schedule trigger or notification topic.\n\n### schedule\n\nTo trigger an execution on a recurring schedule, wrap the main function in a schedule decorator:\n\n```python\n@schedule(expression="rate(1 hour)")\ndef main(data):\n    MyTask()\n```\n\nThe expression keyword argument can either be a cron or rate expression. See the [documentation for ScheduledEvents](https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html) for more details on the expression format.\n\n### subscribe\n\nTo trigger an execution when a message is published to an SNS topic from another project, wrap the function in a subscribe decorator:\n\n```python\n@subscribe(project="other-project")\ndef main(data):\n    MyTask()\n```\n\nBy default, this will subscribe to successful execution events published for the main state machine in the project named other-project. The project keyword argument refers to the folder name of another project.\n\nOther keyword arguments:\n\n- `state_machine`: The identifier of a state machine function in the source project. By default this is "main", but this allows subscriptions to other exported state machines.\n- `status`: One of "success" (default) or "failure". This represents the execution status of the source state machine execution. It will be used to select which of the two SNS topics from the source project to subscribe to.\n\nFluxio will take the project and state_machine arguments and pick the right ImportValue based on the CloudFormation stack and environment.\n\nIf you want to subscribe to an explicit SNS topic that has been exported from another stack outside of Fluxio, you can provide the `topic_arn_import_value` keyword argument instead:\n\n```python\n@subscribe(topic_arn_import_value="${Environment}-my-topic-arn")\ndef main(data):\n    MyTask()\n```\n\nThe value for this argument can be a simple string but can also include any CloudFormation substitution variables that you have access to in the template since the string will be wrapped in [`!Sub`](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-sub.html). These include parameters and resources references.\n\n### export\n\nTo explicitly "export" a state machine, wrap the function in an export decorator:\n\n```python\n@export()\ndef some_state_machine(data):\n    MyTask()\n```\n\nAn "exported" state machine gets its own CloudFormation template and can be directly executed.\n\nYou only need to use the export decorator if:\n\n- You have multiple state machines in a project.py file\n- AND one of them is nested in another\n- AND you want to be able to directly execute the nested state machine\n- AND the nested state machine function isn\'t already wrapped in schedule or subscribe (those decorators cause the state machine to be exported automatically)\n',
    'author': 'Jonathan Drake',
    'author_email': 'jdrake@narrativescience.com',
    'maintainer': None,
    'maintainer_email': None,
    'url': None,
    'packages': packages,
    'package_data': package_data,
    'install_requires': install_requires,
    'python_requires': '>=3.6,<4.0',
}


setup(**setup_kwargs)
