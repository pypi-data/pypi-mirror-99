{"cells": [{"source": ["# Example: Model definition and training\n", "\n", "\n", "The following example shows how to model a two-layer neural network step by step and train from scratch In a few lines of Python code.\n", "\n", "```\n", "import jittor as jt\n", "import numpy as np\n", "from jittor import nn, Module, init\n", "```\n", "\n", "The following code defines our model, which is a two-layer neural network. The size of hidden layer is 10. and the activation function is relu.\n", "\n", "```\n", "### model define\n", "\n", "class Model(Module):\n", "    def __init__(self):\n", "        self.layer1 = nn.Linear(1, 10)\n", "        self.relu = nn.ReLU()\n", "        self.layer2 = nn.Linear(10, 1)\n", "    def execute (self,x) :\n", "        x = self.layer1(x)\n", "        x = self.relu(x)\n", "        x = self.layer2(x)\n", "        return x\n", "```\n", "\n", "At last, this model is trained from scratch. A simple gradient descent is used, and the loss function is L2 distance. The training process is asynchronous for efficiency. jittor calculates the gradients and applies graph- and operator-level optimizations via **unify IR graph** and **jit analyzer**.\n", "In this example, multiple optimizations can be used, including: **operator fusion**, the activation function and loss function can be fused into the first and second linear layers; Three meta-operators in matrix multiplication could also be fused. **Parallelism**, it can improve performance of compute-intensive operations on modern multi-core CPUs and GPUs. The operator fusion is a graph-level optimization, and parallelism can be achieved in both graph-level and operator-level.\n", "\n", "```\n", "np.random.seed(0)\n", "jt.set_seed(3)\n", "n = 1000\n", "batch_size = 50\n", "base_lr = 0.05\n", "# we need to stop grad of global value to prevent memory leak\n", "lr = jt.float32(base_lr).name(\"lr\").stop_grad()\n", "\n", "def get_data(n):\n", "    for i in range(n):\n", "        x = np.random.rand(batch_size, 1)\n", "        y = x*x\n", "        yield jt.float32(x), jt.float32(y)\n", "\n", "model = Model()\n", "learning_rate = 0.1\n", "optim = nn.SGD (model.parameters(), learning_rate)\n", "\n", "for i,(x,y) in enumerate(get_data(n)):\n", "    pred_y = model(x)\n", "    loss = jt.sqr(pred_y - y)\n", "    loss_mean = loss.mean()\n", "    optim.step (loss_mean)\n", "    print(f\"step {i}, loss = {loss_mean.data.sum()}\")\n", "\n", "assert loss_mean.data < 0.005\n", "```"], "metadata": {}, "cell_type": "markdown"}], "nbformat": 4, "nbformat_minor": 2, "metadata": {}}