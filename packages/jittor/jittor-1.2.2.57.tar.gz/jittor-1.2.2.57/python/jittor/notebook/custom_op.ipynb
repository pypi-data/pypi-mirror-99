{"cells": [{"source": ["# Custom Op: write your operator with C++ and CUDA and JIT compile it\n", "\n", "\n", "> NOTE: This tutorial is still working in progress\n", "\n", "In this tutorial, we will show:\n", "\n", "1. how to write your operator with C++ and CUDA and JIT compile it\n", "2. execute your custom operation\n", "\n", "If you want to implement a very simple op with few lines of code, please use code op, please see `help(jt.code)`.\n", "custom_op is used for implement a complicated op. The capabilities of custom_op and built-in operations are exactly the same."], "metadata": {}, "cell_type": "markdown"}, {"source": ["import jittor as jt\n", "\n", "header =\"\"\"\n", "#pragma once\n", "#include \"op.h\"\n", "\n", "namespace jittor {\n", "\n", "struct CustomOp : Op {\n", "    Var* output;\n", "    CustomOp(NanoVector shape, NanoString dtype=ns_float);\n", "    \n", "    const char* name() const override { return \"custom\"; }\n", "    DECLARE_jit_run;\n", "};\n", "\n", "} // jittor\n", "\"\"\"\n", "\n", "src = \"\"\"\n", "#include \"var.h\"\n", "#include \"custom_op.h\"\n", "\n", "namespace jittor {\n", "#ifndef JIT\n", "CustomOp::CustomOp(NanoVector shape, NanoString dtype) {\n", "    flags.set(NodeFlags::_cuda, 1);\n", "    flags.set(NodeFlags::_cpu, 1);\n", "    output = create_output(shape, dtype);\n", "}\n", "\n", "void CustomOp::jit_prepare() {\n", "    add_jit_define(\"T\", output->dtype());\n", "}\n", "\n", "#else // JIT\n", "#ifdef JIT_cpu\n", "void CustomOp::jit_run() {\n", "    index_t num = output->num;\n", "    auto* __restrict__ x = output->ptr<T>();\n", "    for (index_t i=0; i<num; i++)\n", "        x[i] = (T)i;\n", "}\n", "#else\n", "// JIT_cuda\n", "__global__ void kernel(index_t n, T *x) {\n", "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n", "    int stride = blockDim.x * gridDim.x;\n", "    for (int i = index; i < n; i += stride)\n", "        x[i] = (T)-i;\n", "}\n", "\n", "void CustomOp::jit_run() {\n", "    index_t num = output->num;\n", "    auto* __restrict__ x = output->ptr<T>();\n", "    int blockSize = 256;\n", "    int numBlocks = (num + blockSize - 1) / blockSize;\n", "    kernel<<<numBlocks, blockSize>>>(num, x);\n", "}\n", "#endif // JIT_cpu\n", "#endif // JIT\n", "\n", "} // jittor\n", "\"\"\"\n", "\n", "my_op = jt.compile_custom_op(header, src, \"custom\", warp=False)"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["Let's check the result of this op."], "metadata": {}, "cell_type": "markdown"}, {"source": ["# run cpu version\n", "jt.flags.use_cuda = 0\n", "a = my_op([3,4,5], 'float').fetch_sync()\n", "assert (a.flatten() == range(3*4*5)).all()\n", "\n", "if jt.compiler.has_cuda:\n", "    # run cuda version\n", "    jt.flags.use_cuda = 1\n", "    a = my_op([3,4,5], 'float').fetch_sync()\n", "    assert (-a.flatten() == range(3*4*5)).all()"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": null}], "nbformat": 4, "nbformat_minor": 2, "metadata": {}}