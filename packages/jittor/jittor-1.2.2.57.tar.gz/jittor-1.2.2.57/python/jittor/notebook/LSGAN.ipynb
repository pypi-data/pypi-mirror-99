{"cells": [{"source": ["```\n", "# \u6587\u4ef6\u7ec4\u7ec7\n", "\u6839\u76ee\u5f55\n", "|----data\n", "     |----celebA_train\n", "     |    |----imgs\n", "     |----celebA_eval\n", "     |    |----imgs\n", "```"], "metadata": {}, "cell_type": "markdown"}, {"source": ["import jittor as jt\n", "from jittor import nn, Module, init\n", "from jittor.dataset.mnist import MNIST\n", "from jittor.dataset.dataset import ImageFolder\n", "import jittor.transform as transform\n", "import os\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "\n", "# \u901a\u8fc7use_cuda\u8bbe\u7f6e\u5728GPU\u4e0a\u8fdb\u884c\u8bad\u7ec3\n", "jt.flags.use_cuda = 1\n", "\n", "class generator(Module):\n", "    def __init__(self, dim=3):\n", "        super(generator, self).__init__()\n", "        self.fc = nn.Linear(1024, 7*7*256)\n", "        self.fc_bn = nn.BatchNorm(256)\n", "        self.deconv1 = nn.ConvTranspose(256, 256, 3, 2, 1, 1)\n", "        self.deconv1_bn = nn.BatchNorm(256)\n", "        self.deconv2 = nn.ConvTranspose(256, 256, 3, 1, 1)\n", "        self.deconv2_bn = nn.BatchNorm(256)\n", "        self.deconv3 = nn.ConvTranspose(256, 256, 3, 2, 1, 1)\n", "        self.deconv3_bn = nn.BatchNorm(256)\n", "        self.deconv4 = nn.ConvTranspose(256, 256, 3, 1, 1)\n", "        self.deconv4_bn = nn.BatchNorm(256)\n", "        self.deconv5 = nn.ConvTranspose(256, 128, 3, 2, 1, 1)\n", "        self.deconv5_bn = nn.BatchNorm(128)\n", "        self.deconv6 = nn.ConvTranspose(128, 64, 3, 2, 1, 1)\n", "        self.deconv6_bn = nn.BatchNorm(64)\n", "        self.deconv7 = nn.ConvTranspose(64 , dim, 3, 1, 1)\n", "        self.relu = nn.ReLU()\n", "        self.tanh = nn.Tanh()\n", "\n", "    def execute(self, input):\n", "        x = self.fc(input).reshape((input.shape[0], 256, 7, 7))\n", "        x = self.relu(self.fc_bn(x))\n", "        x = self.relu(self.deconv1_bn(self.deconv1(x)))\n", "        x = self.relu(self.deconv2_bn(self.deconv2(x)))\n", "        x = self.relu(self.deconv3_bn(self.deconv3(x)))\n", "        x = self.relu(self.deconv4_bn(self.deconv4(x)))\n", "        x = self.relu(self.deconv5_bn(self.deconv5(x)))\n", "        x = self.relu(self.deconv6_bn(self.deconv6(x)))\n", "        x = self.tanh(self.deconv7(x))\n", "        return x\n", "\n", "\n", "class discriminator(nn.Module):\n", "    def __init__(self, dim=3):\n", "        super(discriminator, self).__init__()\n", "        self.conv1 = nn.Conv(dim, 64, 5, 2, 2)\n", "        self.conv2 = nn.Conv(64, 128, 5, 2, 2)\n", "        self.conv2_bn = nn.BatchNorm(128)\n", "        self.conv3 = nn.Conv(128, 256, 5, 2, 2)\n", "        self.conv3_bn = nn.BatchNorm(256)\n", "        self.conv4 = nn.Conv(256, 512, 5, 2, 2)\n", "        self.conv4_bn = nn.BatchNorm(512)\n", "        self.fc = nn.Linear(512*7*7, 1)\n", "        self.leaky_relu = nn.Leaky_relu()\n", "\n", "    def execute(self, input):\n", "        x = self.leaky_relu(self.conv1(input), 0.2)\n", "        x = self.leaky_relu(self.conv2_bn(self.conv2(x)), 0.2)\n", "        x = self.leaky_relu(self.conv3_bn(self.conv3(x)), 0.2)\n", "        x = self.leaky_relu(self.conv4_bn(self.conv4(x)), 0.2)\n", "        x = x.reshape((x.shape[0], 512*7*7))\n", "        x = self.fc(x)\n", "        return x"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["def ls_loss(x, b):\n", "    mini_batch = x.shape[0]\n", "    y_real_ = jt.ones((mini_batch,))\n", "    y_fake_ = jt.zeros((mini_batch,))\n", "    if b:\n", "        return (x-y_real_).sqr().mean()\n", "    else:\n", "        return (x-y_fake_).sqr().mean()"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["# \u4f7f\u7528 MNIST \u6216\u8005 CelebA\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\n", "task = \"MNIST\"\n", "# task = \"CelebA\"\n", "# \u6279\u5927\u5c0f\n", "batch_size = 128\n", "# \u5b66\u4e60\u7387\n", "lr = 0.0002\n", "# \u8bad\u7ec3\u8f6e\u6570\n", "train_epoch = 20 if task==\"MNIST\" else 50\n", "# \u8bad\u7ec3\u56fe\u50cf\u6807\u51c6\u5927\u5c0f\n", "img_size = 112\n", "# Adam\u4f18\u5316\u5668\u53c2\u6570\n", "betas = (0.5,0.999)\n", "# \u6570\u636e\u96c6\u56fe\u50cf\u901a\u9053\u6570\uff0cMNIST\u4e3a1\uff0cCelebA\u4e3a3\n", "dim = 1 if task==\"MNIST\" else 3\n", "# \u7ed3\u679c\u56fe\u7247\u5b58\u50a8\u8def\u5f84\n", "save_path = \"./results_img\""], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["G = generator (dim)\n", "D = discriminator (dim)\n", "G_optim = nn.Adam(G.parameters(), lr, betas=betas)\n", "D_optim = nn.Adam(D.parameters(), lr, betas=betas)"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["if task==\"MNIST\":\n", "    transform = transform.Compose([\n", "        transform.Resize(size=img_size),\n", "        transform.Gray(),\n", "        transform.ImageNormalize(mean=[0.5], std=[0.5]),\n", "    ])\n", "    train_loader = MNIST(train=True, transform=transform).set_attrs(batch_size=batch_size, shuffle=True)\n", "    eval_loader = MNIST(train=False, transform = transform).set_attrs(batch_size=batch_size, shuffle=True)\n", "elif task==\"CelebA\":\n", "    transform = transform.Compose([\n", "        transform.Resize(size=img_size),\n", "        transform.ImageNormalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n", "    ])\n", "    train_dir = './data/celebA_train'\n", "    train_loader = ImageFolder(train_dir).set_attrs(transform=transform, batch_size=batch_size, shuffle=True)\n", "    eval_dir = './data/celebA_eval'\n", "    eval_loader = ImageFolder(eval_dir).set_attrs(transform=transform, batch_size=batch_size, shuffle=True)"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["def train(epoch):\n", "    for batch_idx, (x_, target) in enumerate(train_loader):\n", "        mini_batch = x_.shape[0]\n", "        # train discriminator\n", "        D_result = D(x_)\n", "        D_real_loss = ls_loss(D_result, True)\n", "        z_ = jt.init.gauss((mini_batch, 1024), 'float')\n", "        G_result = G(z_)\n", "        D_result_ = D(G_result)\n", "        D_fake_loss = ls_loss(D_result_, False)\n", "        D_train_loss = D_real_loss + D_fake_loss\n", "        D_train_loss.sync()\n", "        D_optim.step(D_train_loss)\n", "\n", "        # train generator\n", "        z_ = jt.init.gauss((mini_batch, 1024), 'float')\n", "        G_result = G(z_)\n", "        D_result = D(G_result)\n", "        G_train_loss = ls_loss(D_result, True)\n", "        G_train_loss.sync()\n", "        G_optim.step(G_train_loss)\n", "        if (batch_idx%100==0):\n", "            print(\"train: batch_idx\",batch_idx,\"epoch\",epoch)\n", "            print('  D training loss =', D_train_loss.data.mean())\n", "            print('  G training loss =', G_train_loss.data.mean())\n", "\n", "def validate(epoch):\n", "    D_losses = []\n", "    G_losses = []\n", "    G.eval()\n", "    D.eval()\n", "    for batch_idx, (x_, target) in enumerate(eval_loader):\n", "        mini_batch = x_.shape[0]\n", "        \n", "        # calculation discriminator loss\n", "        D_result = D(x_)\n", "        D_real_loss = ls_loss(D_result, True)\n", "        z_ = jt.init.gauss((mini_batch, 1024), 'float')\n", "        G_result = G(z_)\n", "        D_result_ = D(G_result)\n", "        D_fake_loss = ls_loss(D_result_, False)\n", "        D_train_loss = D_real_loss + D_fake_loss\n", "        D_losses.append(D_train_loss.data.mean())\n", "\n", "        # calculation generator loss\n", "        z_ = jt.init.gauss((mini_batch, 1024), 'float')\n", "        G_result = G(z_)\n", "        D_result = D(G_result)\n", "        G_train_loss = ls_loss(D_result, True)\n", "        G_losses.append(G_train_loss.data.mean())\n", "    G.train()\n", "    D.train()\n", "    print(\"validate: epoch\",epoch)\n", "    print('  D validate loss =', np.array(D_losses).mean())\n", "    print('  G validate loss =', np.array(G_losses).mean())"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["if not os.path.exists(save_path):\n", "    os.mkdir(save_path)\n", "fixed_z_ = jt.init.gauss((5 * 5, 1024), 'float')\n", "def save_result(num_epoch, G , path = 'result.png'):\n", "    \"\"\"Use the current generator to generate 5*5 pictures and store them.\n", "\n", "    Args:\n", "        num_epoch(int): current epoch\n", "        G(generator): current generator\n", "        path(string): storage path of result image\n", "    \"\"\"\n", "\n", "    z_ = fixed_z_\n", "    G.eval()\n", "    test_images = G(z_)\n", "    G.train()\n", "    size_figure_grid = 5\n", "    fig, ax = plt.subplots(size_figure_grid, size_figure_grid, figsize=(5, 5))\n", "    for i in range(size_figure_grid):\n", "        for j in range(size_figure_grid):\n", "            ax[i, j].get_xaxis().set_visible(False)\n", "            ax[i, j].get_yaxis().set_visible(False)\n", "\n", "    for k in range(5*5):\n", "        i = k // 5\n", "        j = k % 5\n", "        ax[i, j].cla()\n", "        if task==\"MNIST\":\n", "            ax[i, j].imshow((test_images[k, 0].data+1)/2, cmap='gray')\n", "        else:\n", "            ax[i, j].imshow((test_images[k].data.transpose(1, 2, 0)+1)/2)\n", "\n", "    label = 'Epoch {0}'.format(num_epoch)\n", "    fig.text(0.5, 0.04, label, ha='center')\n", "    plt.savefig(path)\n", "    plt.show()"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["for epoch in range(train_epoch):\n", "    print ('number of epochs', epoch)\n", "    train(epoch)\n", "    validate(epoch)\n", "    result_img_path = './results_img/' + task + str(epoch) + '.png'\n", "    save_result(epoch, G, path=result_img_path)"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": null}], "nbformat": 4, "nbformat_minor": 2, "metadata": {}}