{"cells": [{"source": ["# \u81ea\u5b9a\u4e49\u7b97\u5b50\uff1a\u4f7f\u7528C ++\u548cCUDA\u7f16\u5199\u60a8\u7684\u7b97\u5b50\uff0c\u5e76\u5176\u8fdb\u884c\u5373\u65f6\u7f16\u8bd1\n", "\n", "\n", "\n", "\n", "\n", "> \u6ce8\u610f\uff1a\u672c\u6559\u7a0b\u4ecd\u5728\u6301\u7eed\u66f4\u65b0\u4e2d\n", "\n", "\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u5c55\u793a\uff1a\n", "\n", "1. \u5982\u4f55\u7528C ++\u548cCUDA\u7f16\u5199\u60a8\u7684\u7b97\u5b50\u5e76\u5bf9\u5176\u8fdb\u884c\u5373\u65f6\u7f16\u8bd1\n", "2. \u8fd0\u884c\u60a8\u7684\u81ea\u5b9a\u4e49\u7b97\u5b50\n", "\n", "\u5982\u679c\u60a8\u60f3\u7528\u51e0\u884c\u4ee3\u7801\u6765\u5b9e\u73b0\u4e00\u4e2a\u975e\u5e38\u7b80\u5355\u7684\u7b97\u5b50\uff0c\u8bf7\u4f7f\u7528code\u8fd0\u7b97\uff0c\u8bf7\u53c2\u9605`help(jt.code)`.\n", "custom_op\u7528\u4e8e\u5b9e\u73b0\u590d\u6742\u7684\u7b97\u5b50\u3002 custom_op\u548c\u5185\u7f6e\u8fd0\u7b97\u7684\u529f\u80fd\u5b8c\u5168\u76f8\u540c\u3002"], "metadata": {}, "cell_type": "markdown"}, {"source": ["import jittor as jt\n", "\n", "header =\"\"\"\n", "#pragma once\n", "#include \"op.h\"\n", "\n", "namespace jittor {\n", "\n", "struct CustomOp : Op {\n", "    Var* output;\n", "    CustomOp(NanoVector shape, NanoString dtype=ns_float);\n", "    \n", "    const char* name() const override { return \"custom\"; }\n", "    DECLARE_jit_run;\n", "};\n", "\n", "} // jittor\n", "\"\"\"\n", "\n", "src = \"\"\"\n", "#include \"var.h\"\n", "#include \"custom_op.h\"\n", "\n", "namespace jittor {\n", "#ifndef JIT\n", "CustomOp::CustomOp(NanoVector shape, NanoString dtype) {\n", "    flags.set(NodeFlags::_cuda, 1);\n", "    flags.set(NodeFlags::_cpu, 1);\n", "    output = create_output(shape, dtype);\n", "}\n", "\n", "void CustomOp::jit_prepare() {\n", "    add_jit_define(\"T\", output->dtype());\n", "}\n", "\n", "#else // JIT\n", "#ifdef JIT_cpu\n", "void CustomOp::jit_run() {\n", "    index_t num = output->num;\n", "    auto* __restrict__ x = output->ptr<T>();\n", "    for (index_t i=0; i<num; i++)\n", "        x[i] = (T)i;\n", "}\n", "#else\n", "// JIT_cuda\n", "__global__ void kernel(index_t n, T *x) {\n", "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n", "    int stride = blockDim.x * gridDim.x;\n", "    for (int i = index; i < n; i += stride)\n", "        x[i] = (T)-i;\n", "}\n", "\n", "void CustomOp::jit_run() {\n", "    index_t num = output->num;\n", "    auto* __restrict__ x = output->ptr<T>();\n", "    int blockSize = 256;\n", "    int numBlocks = (num + blockSize - 1) / blockSize;\n", "    kernel<<<numBlocks, blockSize>>>(num, x);\n", "}\n", "#endif // JIT_cpu\n", "#endif // JIT\n", "\n", "} // jittor\n", "\"\"\"\n", "\n", "my_op = jt.compile_custom_op(header, src, \"custom\", warp=False)"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["\u8ba9\u6211\u4eec\u67e5\u770b\u4e00\u4e0b\u8fd9\u4e2a\u8fd0\u7b97\u7684\u7ed3\u679c\u3002"], "metadata": {}, "cell_type": "markdown"}, {"source": ["# run cpu version\n", "jt.flags.use_cuda = 0\n", "a = my_op([3,4,5], 'float').fetch_sync()\n", "assert (a.flatten() == range(3*4*5)).all()\n", "\n", "if jt.compiler.has_cuda:\n", "    # run cuda version\n", "    jt.flags.use_cuda = 1\n", "    a = my_op([3,4,5], 'float').fetch_sync()\n", "    assert (-a.flatten() == range(3*4*5)).all()"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": null}], "nbformat": 4, "nbformat_minor": 2, "metadata": {}}